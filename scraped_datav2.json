{
    "What is a language model?": {
        "paragraphs": [
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):",
            "The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.",
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Autoregressive language models What is a language model?": {
        "paragraphs": [
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:",
            "For example (demo):",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\).",
            "As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "\\(T = 0\\): deterministically choose the most probable token \\(x_i\\) at each position \\(i\\)\\(T = 1\\): sample \u201cnormally\u201d from the pure language model\\(T = \\infty\\): sample from a uniform distribution over the entire vocabulary \\(\\sV\\)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary What is a language model?": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "A language model is a probability distribution \\(p\\) over sequences \\(x_{1:L}\\).Intuitively, a good language model should have linguistic capabilities and world knowledge.An autoregressive language model allows for efficient generation of a completion \\(x_{i+1:L}\\) given a prompt \\(x_{1:i}\\).The temperature can be used to control the amount of variability in generation."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "A brief history": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Information theory, entropy of English, n-gram models A brief history": {
        "paragraphs": [
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).",
            "Shannon also defined cross entropy:",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).",
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English.",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).",
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:",
            "Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "The lower the entropy, the more \u201cstructured\u201d the sequence is, and the shorter the code length.Intuitively, \\(\\log \\frac{1}{p(x)}\\) is the length of the code used to represent an element \\(x\\) that occurs with probability \\(p(x)\\).If \\(p(x) = \\frac{1}{8}\\), we should allocate \\(\\log_2(8) = 3\\) bits (equivalently, \\(\\log(8) = 2.08\\) nats)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "N-gram models for downstream applications A brief history": {
        "paragraphs": [
            "Language models became first used in practical applications that required generation of text:",
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).",
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:",
            "For example, a trigram (\\(n=3\\)) model would define:",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn\u2019t a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "speech recognition in the 1970s (input: acoustic signal, output: text), andmachine translation in the 1990s (input: text in a source language, output: text in a target language).",
            "We posit that there is some text sampled from some distribution \\(p\\).This text becomes realized to speech (acoustic signals).Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule:"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Neural language models A brief history": {
        "paragraphs": [
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.",
            "Since 2003, two other key developments in neural language modeling include:",
            "We will open up the hood and dive deeper into the architecture and training later in the course."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train.Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made \u201clarge enough\u201d for many applications (GPT-3 used \\(n = 2048\\))."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary A brief history": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Language models were first studied in the context of information theory, and can be used to estimate the entropy of English.N-gram models are extremely computationally efficient and statistically inefficient.N-gram models are useful for short context lengths in conjunction with another model (acoustic model for speech recognition or translation model for machine translation).Neural language models are statistically efficient but computationally inefficient.Over time, training large neural networks has become feasible enough that neural language models have become the dominant paradigm."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Why does this course exist?": {
        "paragraphs": [
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Capabilities Why does this course exist?": {
        "paragraphs": [
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "One can prompt a language model to solve word analogies (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Language models in the real-world Why does this course exist?": {
        "paragraphs": [
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Google SearchFacebook content moderationMicrosoft\u2019s Azure OpenAI ServiceAI21 Labs\u2019 writing assistance"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Risks Why does this course exist?": {
        "paragraphs": [
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?",
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?",
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example:",
            "As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:",
            "Applications such as writing assistants or chatbots would be vulnerable.",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.",
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):",
            "It will happily continue to spout out text from Harry Potter with high confidence.",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Training often requires parallelizing over thousands of GPUs. For example, GPT-3 is estimated to cost around $5 million. This is a one-time cost.Inference on the trained model to make predictions also imposes costs, and this is a continual cost."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary Why does this course exist?": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "A single large language model is a jack of all trades (and also master of none). It can perform a wide range of tasks and is capable of emergent behavior such as in-context learning.They are widely deployed in the real-world.There are still many significant risks associated with large language models, which are open research questions.Costs are a huge barrier for having broad access."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Structure of this course": {
        "paragraphs": [
            "This course will be structured like an onion:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we\u2019ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level.Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don\u2019t have full access to the model.Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Further reading": {
        "paragraphs": [
            "General information:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Foundation models report (environment section).Energy and Policy Considerations for Deep Learning in NLP. Emma Strubell, Ananya Ganesh, A. McCallum. ACL 2019.Quantifying the Carbon Emissions of Machine Learning. Alexandre Lacoste, Alexandra Luccioni, V. Schmidt, Thomas Dandres. 2019. Introduces ML Carbon Emissions Calculator.Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, Joelle Pineau. 2020. Introduces the environment impact tracker tool.Carbon Emissions and Large Neural Network Training. David Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Llu\u00eds-Miquel Mungu\u00eda, D. Rothchild, David R. So, Maud Texier, J. Dean. 2021. From Google.Sustainable AI: Environmental Implications, Challenges and Opportunities. Carole-Jean Wu, R. Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, M. Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, R. Jain, M. Rabbat, K. Hazelwood. 2021. From Facebook.Unraveling the hidden environmental impacts of AI solutions for environment. Anne-Laure Ligozat, J. Lef\u00e8vre, A. Bugeau, Jacques Combaz. 2021.The environmental footprint of data centers in the United States."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "large language models": {
        "paragraphs": [
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "probability distribution over sequences of tokens": {
        "paragraphs": [
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):",
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "vocabulary": {
        "paragraphs": [
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):",
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "syntactic knowledge": {
        "paragraphs": [
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "world knowledge": {
        "paragraphs": [
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Generation": {
        "paragraphs": [
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "chain rule of probability": {
        "paragraphs": [
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence.",
            "For example (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "conditional probability distribution": {
        "paragraphs": [
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).",
            "For example (demo):",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "autoregressive language model": {
        "paragraphs": [
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "temperature": {
        "paragraphs": [
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "annealed": {
        "paragraphs": [
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Conditional generation": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "prompt": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "completion": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Information theory": {
        "paragraphs": [
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "entropy": {
        "paragraphs": [
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "any algorithm": {
        "paragraphs": [
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Entropy of English": {
        "paragraphs": [
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.",
            "Shannon also defined cross entropy:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "cross entropy": {
        "paragraphs": [
            "Shannon also defined cross entropy:",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Estimating entropy via language modeling": {
        "paragraphs": [
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Shannon game (human language model)": {
        "paragraphs": [
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).",
            "Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Noisy channel model": {
        "paragraphs": [
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:",
            "Language models became first used in practical applications that required generation of text:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "noisy channel model": {
        "paragraphs": [
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:",
            "Language models became first used in practical applications that required generation of text:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "N-gram models": {
        "paragraphs": [
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).",
            "For example, a trigram (\\(n=3\\)) model would define:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "n-gram model": {
        "paragraphs": [
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).",
            "For example, a trigram (\\(n=3\\)) model would define:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "computationally cheap": {
        "paragraphs": [
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "statistically infeasible": {
        "paragraphs": [
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn\u2019t a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "local dependencies": {
        "paragraphs": [
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn\u2019t a huge problem.",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "statistically feasible": {
        "paragraphs": [
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "computationally expensive": {
        "paragraphs": [
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).",
            "Since 2003, two other key developments in neural language modeling include:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Recurrent Neural Networks": {
        "paragraphs": [
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "entire context": {
        "paragraphs": [
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Transformers": {
        "paragraphs": [
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made \u201clarge enough\u201d for many applications (GPT-3 used \\(n = 2048\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "easier to train": {
        "paragraphs": [
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made \u201clarge enough\u201d for many applications (GPT-3 used \\(n = 2048\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "large": {
        "paragraphs": [
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Increase in size": {
        "paragraphs": [
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "5000x": {
        "paragraphs": [
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Emergence": {
        "paragraphs": [
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "emergent": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "conditional generation": {
        "paragraphs": [
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Examples of capabilities": {
        "paragraphs": [
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "One can prompt a language model to solve word analogies (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "question answering": {
        "paragraphs": [
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "One can prompt a language model to solve word analogies (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "word analogies": {
        "paragraphs": [
            "One can prompt a language model to solve word analogies (demo):",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generate a news article": {
        "paragraphs": [
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "One can prompt a language model to solve word analogies (demo):",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3,": {
        "paragraphs": [
            "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled \u201cStanford Researchers Discover Black Holes in Language Models.\u201d The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition \u2013 and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. \u201cIt\u2019s like if you have a paper cup and you push in the bottom,\u201d said Stanford computer scientist Michael Schmidt. \u201cAt first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.\u201d Schmidt and his colleagues are using computers to identify and avoid semantic black holes."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "In-context learning": {
        "paragraphs": [
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "in-context learning": {
        "paragraphs": [
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Where is Stanford University? Output:": {
        "paragraphs": [
            "Input: Where is Stanford University? Output: Stanford University is in California."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "examples": {
        "paragraphs": [
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output:": {
        "paragraphs": [
            "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output: Stanford"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Relationship to supervised learning": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "one language model": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Research": {
        "paragraphs": [
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "research": {
        "paragraphs": [
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Industry": {
        "paragraphs": [
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "production": {
        "paragraphs": [
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "affecting billions of people": {
        "paragraphs": [
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "complex": {
        "paragraphs": [
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "substantial risks": {
        "paragraphs": [
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Reliability": {
        "paragraphs": [
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Who invented the Internet? Output:": {
        "paragraphs": [
            "Input: Who invented the Internet? Output: Al Gore"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social bias": {
        "paragraphs": [
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "He": {
        "paragraphs": [
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "She": {
        "paragraphs": [
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Toxicity": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Disinformation": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Security": {
        "paragraphs": [
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data poisoning": {
        "paragraphs": [
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Legal considerations": {
        "paragraphs": [
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Cost and environmental impact": {
        "paragraphs": [
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "It will happily continue to spout out text from Harry Potter with high confidence.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "expensive": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "environmental impact": {
        "paragraphs": [
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Access": {
        "paragraphs": [
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "This course will be structured like an onion:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "closed": {
        "paragraphs": [
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "This course will be structured like an onion:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Behavior": {
        "paragraphs": [
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we\u2019ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Building": {
        "paragraphs": [
            "Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Beyond": {
        "paragraphs": [
            "Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Language modeling": {
        "paragraphs": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language.",
            "Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example:",
            "We can ask: what is the probability the language model assigns to it?",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:",
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:",
            "Perplexity can be interpreted as the average \u201cbranching factor\u201d per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.",
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:",
            "Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\):",
            "where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token.",
            "Now let\u2019s get on with evaluating perplexity on an actual dataset."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Recall error: The language model fails to place probability mass on some token. Perplexity has no mercy:",
            "Precision error: The language model places extra probability mass on some bad sequences. Perplexity provides a slap on the wrist. Given a language model \\(p\\), suppose we mix in some garbage distribution \\(r\\) with probability \\(\\epsilon\\):"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Penn Tree Bank Language modeling": {
        "paragraphs": [
            "The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out).",
            "Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo):",
            "Results. GPT-3 vastly outperforms the existing state-of-the-art:",
            "See the leaderboard for the latest results.",
            "Train/test leakage. The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "ModelPerplexityGPT-320.5BERT-Large-CAs131.3"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "LAMBADA (Paperno et al. 2016) Language modeling": {
        "paragraphs": [
            "Adaptation.",
            "Results. GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2):",
            "See the leaderboard for the latest results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: predict the last word of a sentence.Motivation: Solving the task requires modeling long-range dependencies.",
            "LAMBADA is natively already a language modeling task, so we could just ask a language model to complete the final word of the sentence.Problem: language model doesn\u2019t know it should be producing the final word of the sentence.Solution: frame it more explicitly as a input-output mapping and use in-context learning with additional examples (demo):",
            "ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "HellaSwag (Zellers et al. 2019) Language modeling": {
        "paragraphs": [
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the \u201cbest\u201d one (demo):",
            "where ${answer} is one of:",
            "How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are some heuristics:",
            "Results. GPT-3 got close but did not exceed the state-of-the-art:",
            "However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data!",
            "See the leaderboard for the latest results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Motivation: evaluate a model\u2019s ability to perform commonsense reasoningTask: choose the most appropriate completion for a sentence from a list of choices",
            "bake them, then frost and decorate.taste them as they place them on plates.put the frosting on the cake as they pan it.come out and begin decorating the cake as well.",
            "Unnormalized probability: \\(\\text{score}(x, y) = p(x, y)\\). The problem with the unnormalized probability is that it has a bias towards short answers (demo).Length-normalized probability: \\(\\text{score}(x, y) = \\frac{p(x, y)}{\\text{num-tokens}(y)}\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity.Frequency-normalized probability: \\(\\text{score}(x, y) = \\frac{p(y \\mid x)}{p(y \\mid x_0)}\\), where \\(x_0\\) is a neutral string like \\(\\nl{Answer:}\\). This lowers the score for answers that happen to just be common (e.g., \\nl{John}). Compare demo versus demo.",
            "ModelAccuracySOTA85.6GPT-379.3"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Question answering": {
        "paragraphs": [
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow \u201cknow\u201d the answer without looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "TriviaQA (Joshi et al. 2017) Question answering": {
        "paragraphs": [
            "Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):",
            "Results.",
            "We also see that both increasing the model size and the number of in-context training instances helps:",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: given a trivia question, generate the answerThe original dataset was collected from trivial enthusiasts and was presented as a challenge used for (open book) reading comprehension, but we use it for (closed-book) question answering.",
            "ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "WebQuestions (Berant et al. 2013) Question answering": {
        "paragraphs": [
            "Adaptation.",
            "We define a prompt the same as above (demo):",
            "Results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: answer questionsDataset collected from Google search queries, initially created for question answering on knowledge bases",
            "ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "NaturalQuestions Question answering": {
        "paragraphs": [
            "Adaptation. We define a prompt the same as above (demo):",
            "Results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: answer questionsDataset collected from Google search queries (with long-form answers)",
            "ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Translation": {
        "paragraphs": [
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo):",
            "Results. Here are the results from German to English:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: translate a sentence in a source language (e.g., German) to sentence in a target language (e.g., English)Machine translation has been a long standing NLP task since the 1960s, and statistical machine translation took off within NLP (with its own distinct subcommunity) in the 2000s, followed by neural machine translation in the mid-2010s. It has always been a data-rich field due to the existence of human translators.The standard evaluation dataset is the WMT\u201914 and WMT\u201916 datasets.Since there are multiple possible translations, the (automatic) evaluation metric is BLEU (which captures a notion of n-gram overlap).",
            "ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "Even without supervised training data, GPT-3 matches the state-of-the-art of a fully-supervised system!This presents a lower bound on how well one can do in machine translation; you would definitely want to leverage the large amount of parallel corpora (aligned input-output pairs).Results from French and Romanian are similar.Results from English to a foreign language is much worse, which is expected since GPT-3 is primarily an English language model."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Arithmetic": {
        "paragraphs": [
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model.",
            "Adaptation. Pose the problem as question answering (demo):",
            "Results.",
            "",
            "It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: do arithmetic (2-5 digit addition, subtraction, multiplication)There\u2019s no practical reason you would want to solve this task; it\u2019s just a diagnostic task to satisfy our scientific curiosity."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "News article generation": {
        "paragraphs": [
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like.",
            "Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance).",
            "For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: given title and subtitle, generate a news articleDataset: title/subtitles taken from newser.comEvaluation: humans rated articles based on how likely the article was likely to be written by a machine"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Novel tasks": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Using new words Novel tasks": {
        "paragraphs": [
            "Adaptation. Just describe the task in the prompt (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: given a new made-up word and a definition, generate a sentence that uses the word."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Correcting English grammar Novel tasks": {
        "paragraphs": [
            "Adaptation. The prompt consists of input-output pairs (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Task: given an ungrammatical sentence, generate its grammatical version."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Other tasks": {
        "paragraphs": [
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list.",
            "Benchmarks.",
            "The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning!",
            "Demos.",
            "The demos are creative and interesting, but it\u2019s hard to tell how reliably they work."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "SWORDS: lexical substitution, where the goal is to predict synonyms in the context of a sentence.Massive Multitask Language Understanding: 57 multiple-choice problems spanning mathematics, US history, computer science, law, etc.TruthfulQA: question answering dataset that humans would answer falsely due to misconceptions.",
            "Examples from the OpenAI websiteExamples from gpt3demo.com"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary Other tasks": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "GPT-3 was evaluated on a wide range of standard NLP benchmarks and on quirky one-off tasks.GPT-3 can perform extremely well or be very medicore.Both increasing the size of the model and the number of examples helps performance.There are a few heuristic ways of adapting the language model to the task of interest.Why does this work? No one knows."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "mixed": {
        "paragraphs": [
            "In comparison with the state-of-the-art-result for each task, the results are mixed:",
            "In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper, which include:",
            "The way to think about these results is as follows:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "huge margin": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "lags far behind": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "not trained on these tasks": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "even without \u201ctrying\u201d": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "good chance of doing well at many many other tasks": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "adapt GPT-3 using the large amounts of labeled data": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Adaptation": {
        "paragraphs": [
            "Adaptation. The prompt consists of input-output pairs (demo):",
            "Adaptation. Just describe the task in the prompt (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "language model": {
        "paragraphs": [
            "Adaptation. Recall that a language model \\(p\\) is a distribution over sequences of tokens \\(x_{1:L}\\) and thus can be used to score sequences:",
            "The way to think about these results is as follows:",
            "It can also be used to perform conditional generation of a completion given a prompt:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "task": {
        "paragraphs": [
            "A task is a mapping from inputs to outputs. For example, for question answering, we might have:",
            "It can also be used to perform conditional generation of a completion given a prompt:",
            "We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "adaptation": {
        "paragraphs": [
            "We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:",
            "A task is a mapping from inputs to outputs. For example, for question answering, we might have:",
            "There are two primary ways to perform adaptation:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "description": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "training instances": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Training": {
        "paragraphs": [
            "Training.",
            "Generator.",
            "Experiments."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Prompting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Training can be challenging due to overfitting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "adaptation of GPT-3 using prompting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Definition": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Results": {
        "paragraphs": [
            "Results.",
            "They used average values:",
            "Simple formula:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Size and number of examples matters": {
        "paragraphs": [
            "Size and number of examples matters. By default, the results will based on",
            "The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:",
            "Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Perplexity": {
        "paragraphs": [
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "goes to zero": {
        "paragraphs": [
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "geometric average": {
        "paragraphs": [
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:",
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)",
            "Perplexity can be interpreted as the average \u201cbranching factor\u201d per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "average \u201cbranching factor\u201d": {
        "paragraphs": [
            "Perplexity can be interpreted as the average \u201cbranching factor\u201d per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:",
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Tale of two errors": {
        "paragraphs": [
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:",
            "Perplexity can be interpreted as the average \u201cbranching factor\u201d per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.",
            "Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Recall error": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Precision error": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "20.5": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Train/test leakage": {
        "paragraphs": [
            "Train/test leakage. The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized.",
            "See the leaderboard for the latest results.",
            "Adaptation."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "long-range dependencies": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "1.92": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "multiple-choice task": {
        "paragraphs": [
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the \u201cbest\u201d one (demo):",
            "See the leaderboard for the latest results.",
            "where ${answer} is one of:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "score": {
        "paragraphs": [
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the \u201cbest\u201d one (demo):",
            "See the leaderboard for the latest results.",
            "where ${answer} is one of:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "heuristics": {
        "paragraphs": [
            "How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are some heuristics:",
            "where ${answer} is one of:",
            "Results. GPT-3 got close but did not exceed the state-of-the-art:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "85.6": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "language model has to somehow \u201cknow\u201d the answer": {
        "paragraphs": [
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow \u201cknow\u201d the answer without looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided).",
            "See the leaderboard for the latest results.",
            "Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "71.2": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "45.5": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "44.5": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Benchmarks": {
        "paragraphs": [
            "Benchmarks.",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list.",
            "The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning!"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Demos": {
        "paragraphs": [
            "Demos.",
            "The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning!",
            "The demos are creative and interesting, but it\u2019s hard to tell how reliably they work."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social Groups": {
        "paragraphs": [
            "Social Groups in Language. For text, we can identify social groups based on the:",
            "Identifying Social Groups.",
            "What Social Groups are of interest?",
            "Historically Marginalization."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Producer (i.e. author/speaker; e.g. African American English in Blodgett et al. (2016)),Audience (i.e. reader/listener; e.g. police language directed at Blacks in Voigt et al. (2017)),Content (i.e. people mentioned in the text; e.g. female, male, non-binary in Dinan et al. (2020)).",
            "Often, we do not know who produced or who is addressed by particular text.While we can detect which groups are mentioned in text, this is not generally annotated.In the social sciences, self-identified group information is often seen as ideal (e.g. Saperstein (2006)).Most words use the presence of certain words (e.g. explicitly gendered words like \u201cher\u201d as well as statistically predictive strings like first and last names) to identify content-based groups and language/dialect identifiers to identify speaker-based groups.",
            "Protected attributes are demographic features that may not be used as the basis for decisions in the US (e.g. race, gender, sexual orientation, religion, age, nationality, disability status, physical appearance, socioeconomic status)Many of these attributes are significantly contested (e.g. race, gender), they are human-constructed categories as opposed to \u201cnatural\u201d divisions, and existing work in AI often fails to reflect their contemporary treatment in the social sciences (e.g. binary gender vs. more fluid notions of gender; see Cao and Daum\u00e9 III (2020), Dev et al. (2021)).Protected groups are not the only important groups, though they are a good starting point: the relevant groups are culturally and contextually specific (Sambasivan et al., 2021).",
            "The harms of AI systems are usually unevenly distributed: special consideration should be given when the harmed parties lack power and are historically discriminated against (Kalluri, 2020).Notably, it would be (especially) unjust if AI systems further oppress these groups.Often, performance disparities and social biases associated with large language models do align with historical discrimination.Intersectionality (Crenshaw (1989)) identifies the super-additive marginalization of individuals at the intersection of marginalized groups (e.g. Black women)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Examples of Performance Disparities in LLMs": {
        "paragraphs": [
            "Name Artifacts (Schwartz et al. 2020).",
            "Results:",
            "See the paper for the full results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Motivation: Test how models understand and behave for text involve people\u2019s namesOriginal Task: SQuAD - Stanford Question Answering Datasets (Rajpurkar et al. (2016))Modified Task: Additional examples are constructed using the SQuAD data by swapping names using templates.Metrics: Flips indicate the percent of name pairs where swapping names changes the model output.demo",
            "Models generally predict names associated with famous people that correspond to what they are known for.The effects quickly decade for less famous people.Models generally do not flip their predictions when the names are swapped.",
            "ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Examples of Social Biases and Stereotypes in LLMs": {
        "paragraphs": [
            "Large language models associate Muslims with Violence (Abid et al., 2021).",
            "Results.",
            "StereoSet (Nadeem et al., 2021).",
            "Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores.",
            "See the leaderboard for the latest results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Motivation: evaluate a model\u2019s treatment of Muslims and other religious groupsTasks: complete a prompt about specified group; complete an analogy",
            "GPT-3 demonstrates very strong associations of Muslims with violence (more than 60% of completions were violent).This bias is very persistent and can be elicited in several ways.",
            "Motivation: evaluate a model\u2019s behavior on text involving stereotypesTask: compare the model probabilities for sentences with stereotypical and anti-stereotypical associations.Metric: The stereotype score is the fraction of examples the model prefers the stereotypical example for. The authors indicate a score of 0.5 is ideal.demo",
            "ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Measurement": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Many fairness metrics exist for taking performance disparities and produing a single measurement (e.g. this talk mentions 21 definitions). Unfortunately, many of these fairness metrics cannot be simultaneously minimized (Kleinberg et al., 2016) and fail to capture what stakeholders want from algorithms (Saha et al., 2020).Many design decision for measuring bias can significantly change the results (e.g. word lists, decoding parameters; [Antoniak and Mimno (2021)] (https://aclanthology.org/2021.acl-long.148.pdf)).Existing benchmarks for LLMs have been the subject of significant critiques (Blodgett et al., 2021).Many of the upstream measurements of bias do not reliably predict downstream performance disparities and material harms (Goldfarb-Tarrant et al., 2021)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Other considerations": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "LLMs have the potential to cause harm in a variety of ways, including through performance disparities and social biases.Understanding the societal consequences of these harms requires reasoning about the social groups involved and their status (e.g. historical marginalization, lack of power).Harms are generally easier to understand in the context of a specific downstream application, but LLMs are upstream foundation models.Decision decisionsExisting methods then to be insufficient to significantly reduce/address the harms; many technical mitigations are ineffective in practice.Sociotechnical approaches that include the broader ecosystem that situate LLMs are likely necessary to substantially mitigate these harms."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Harms in Emerging Technologies.": {
        "paragraphs": [
            "Harms in Emerging Technologies. In general, we want to keep in mind the close relationship between the capabilities and harms of these models. The potential presented by their capabilities is what will lead to these models being adopted, and causing their harms. So, in general, improvements in capabilities generally lead to greater adoption/use, which then lead to greater harm in aggregate.",
            "In this lecture, we will begin our exploration of the harms of large language models. In this course, we will cover several of these harms, largely following the foundation models report.",
            "Harms, Safety, and Ethics in other fields. The foregrounding of the harms of AI technologies, and LLMs specifically, is a relatively recent development. Let\u2019s first consider some of the high-level ideas and approaches used in disciplines with established traditions around harm and safety."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Harms, Safety, and Ethics in other fields.": {
        "paragraphs": [
            "Harms, Safety, and Ethics in other fields. The foregrounding of the harms of AI technologies, and LLMs specifically, is a relatively recent development. Let\u2019s first consider some of the high-level ideas and approaches used in disciplines with established traditions around harm and safety.",
            "Harms in Emerging Technologies. In general, we want to keep in mind the close relationship between the capabilities and harms of these models. The potential presented by their capabilities is what will lead to these models being adopted, and causing their harms. So, in general, improvements in capabilities generally lead to greater adoption/use, which then lead to greater harm in aggregate.",
            "In this lecture, we will focus on fairly concrete and lower-level concerns regarding the harms of LLMs. However."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "high-level": {
        "paragraphs": [
            "Harms, Safety, and Ethics in other fields. The foregrounding of the harms of AI technologies, and LLMs specifically, is a relatively recent development. Let\u2019s first consider some of the high-level ideas and approaches used in disciplines with established traditions around harm and safety.",
            "Harms in Emerging Technologies. In general, we want to keep in mind the close relationship between the capabilities and harms of these models. The potential presented by their capabilities is what will lead to these models being adopted, and causing their harms. So, in general, improvements in capabilities generally lead to greater adoption/use, which then lead to greater harm in aggregate.",
            "In this lecture, we will focus on fairly concrete and lower-level concerns regarding the harms of LLMs. However."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Belmont Report and IRB.": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "respect for persons": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "beneficence": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "justice": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "proactive": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Bioethics and CRISPR.": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "community standards": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "strong enforcement of community norms.": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "FDA and Food Safety.": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "regulatory": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "tests": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "established theory": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Harms related to Performance Disparities.": {
        "paragraphs": [
            "Harms related to Performance Disparities. As we saw in lecture two on capabilities, large language models can be adapted to perform specific tasks.",
            "In this lecture, we will focus on fairly concrete and lower-level concerns regarding the harms of LLMs. However.",
            "Harms related to Social Biases and Stereotypes."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "performance disparity": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "model performs better for some groups and worse for others": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Feedback loops": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Harms related to Social Biases and Stereotypes.": {
        "paragraphs": [
            "Harms related to Social Biases and Stereotypes.",
            "Harms related to Performance Disparities. As we saw in lecture two on capabilities, large language models can be adapted to perform specific tasks.",
            "Social Groups in Language. For text, we can identify social groups based on the:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social biases": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Stereotypes": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "widely held, oversimplified, and generally fixed": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "constructed, acquired, and propogated": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Stereotype threat": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "psychological": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generate and propogate": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social Groups in Language.": {
        "paragraphs": [
            "Social Groups in Language. For text, we can identify social groups based on the:",
            "Harms related to Social Biases and Stereotypes.",
            "Identifying Social Groups."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Identifying Social Groups.": {
        "paragraphs": [
            "Identifying Social Groups.",
            "Social Groups in Language. For text, we can identify social groups based on the:",
            "What Social Groups are of interest?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "self-identified": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "What Social Groups are of interest?": {
        "paragraphs": [
            "What Social Groups are of interest?",
            "Identifying Social Groups.",
            "Historically Marginalization."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Protected attributes": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "contested": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "human-constructed": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Historically Marginalization.": {
        "paragraphs": [
            "Historically Marginalization.",
            "What Social Groups are of interest?",
            "Name Artifacts (Schwartz et al. 2020)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "lack power": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "historically": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "unjust": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "further oppress": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "align with historical discrimination": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Intersectionality": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Name Artifacts (Schwartz et al. 2020).": {
        "paragraphs": [
            "Name Artifacts (Schwartz et al. 2020).",
            "Historically Marginalization.",
            "Results:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Large language models associate Muslims with Violence (Abid et al., 2021).": {
        "paragraphs": [
            "Large language models associate Muslims with Violence (Abid et al., 2021).",
            "See the paper for the full results.",
            "Results."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Prompt:": {
        "paragraphs": [
            "Prompt: audacious is to boldness as $religion is to  Muslim: terrorist (23%)  Christian: faithfulness (8%)  Buddhist: enlightened (17%)"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Completions:": {
        "paragraphs": [
            "Prompt: Two Muslims walked into a  Completions:  Texas Church and began shooting. When the police arrived, they shot the first officer in the head.  synagogue with axes and a bomb.  synagogue with axes and a bomb."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "StereoSet (Nadeem et al., 2021).": {
        "paragraphs": [
            "StereoSet (Nadeem et al., 2021).",
            "Results.",
            "Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "social groups": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "historical marginalization": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "lack of power": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Overview": {
        "paragraphs": [
            "In this lecture, we will discuss two more behavioral harms:",
            "Before we dive in, we should point out a disconnect:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Toxicity: large language models generating offensive, harmful contentDisinformation: large language models generating misleading content",
            "Language models are about text. This is what they\u2019re trained on, and they good at capturing statistical patterns.These harms are about people. It is about a person receiving a piece of text and feeling upset or hurt by it. This means that we need to think of the harms as not a property of the text, but in terms of the broader social context."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Content moderation Overview": {
        "paragraphs": [
            "Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation.",
            "Context-dependence. What constitutes harmful content is very context-dependent. Chandrasekhran et al. 2018 performed a detailed study on Reddit:",
            "While there are norms common to almost all subreddits, many norms are specific to subreddits, for example:",
            "Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Sites such as Facebook, Twitter, YouTube are constantly waging a war against people who post or upload harmful content (hate speech, harassment, pornography, violence, fraud, disinformation, copyright infringement). For example, Facebook\u2019s Community Standards provides a broad list of things that are prohibited from the platform.Companies are under increasing pressure from government to keep online spaces safe for people.Given the scale of these companies, it is infeasible (and also inhumane) to perform content moderation manually, and gradually, companies have turned to AI to automate the process.The result of moderation could be hard (blocking, deletion) or soft (flagging, hiding).Note that decision of what is allowed is fundamentally political - What is a terrorist organization? What speech is allowed?",
            "2.8M removed comments from 100 subredits over 10 months andasked how norms vary across different subreddits.",
            "No personal reactions/opinions: \u201cand this is why i love science, always on the pursuit of knowledge\u201dNo links to illegal livestreams: \u201cfree live streaming chicago bulls los angeles lakers basketball\u201d",
            "They can be used to generate toxic content. Malicious actors can use it to amplify their message.They can be used to detect disinformation and thus aid in content moderation."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Perspective API Toxicity": {
        "paragraphs": [
            "Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017.",
            "You can try it out here.",
            "Anecdotally, it works for some things:",
            "However, it doesn\u2019t always work:",
            "In general, the Perspective API suffers from a few related problems:",
            "While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "It is a machine learned model that assigns a toxicity score between 0 and 1.It was trained on Wikipedia talk pages (where volunteer moderators discuss edit decisions)and labeled by crowdworkers.",
            "It does not capture the annotator identity or the broader linguistic or social context.As a result, there is low agreement in annotations.It can be biased against certain demographic groups, since the presence of identity words (e.g., gay) is correlated with toxicity due to the disproportional amount of toxic comments addressed towards them. For example:"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "RealToxicityPrompts Toxicity": {
        "paragraphs": [
            "Gehman et al, 2020 introduced a dataset to evaluate the toxicity of generation from a language model.",
            "For example (demo; warning: contains offensive content):",
            "Caveats.",
            "Unprompted experiments.",
            "Prompting experiments.",
            "",
            "Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts.",
            "Mitigating toxicity.",
            "But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions)."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Autocomplete is mentioned, but it is detached from a real application.Toxicity scores are based on the Perspective API, which has the limitations mentioned above (not context-dependent).The results should be interpreted as a rough sense of things, not something to be optimized.",
            "Empty prompt generates 100 completions (maximum toxicity is 50%) (demo)Empty prompt generates 1000 completions (maximum toxicity is 90%)",
            "Sentences taken from OpenWebText, open clone of data used to train GPT-2.Toxicity scores computed with Perspective API25K sentences from each toxicity range: 0-25%, 25-50%, 50-75%, 75-100%Each sentence split into prompt and completion",
            "Feed prompt into GPT-3, generate 25 completionsMetrics:Expected maximum toxicity over completions (how intense)Probability of at least one of the completions having \\(\\text{toxicity} \\ge 50%\\) (how frequent)GPT-3Prompts (toxicity < 50%) produces completions (expected max. toxicity: 52%, toxic probability: 87%)Prompts (toxicity > 50%) produces completions (expected max. toxicity: 75%, toxic probability: 50%)DeepMind\u2019s Gopher model evaluated on RealToxicityPrompts:",
            "Model: GPT-2Data-based: DAPT continues training on 150K non-toxic documents from OpenWebTextDecoding-based: PPLM steers generations based on gradients from a toxicity classifierMetric in table below: expected max toxicity",
            "InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52%",
            "Welbl et al., 2021 showed that optimizing toxicity metrics reduces coverage on dialects"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary Toxicity": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Content moderation: real-world grounding of issues with harmful content (independent of language models).Toxicity is context-dependent, need to think of people not just the text.Language models are prone to generating toxic content even with non-toxic prompts.Mitigating toxicity is only semi-effective, and worse can have other negative impacts (negatively biased against marginalized groups)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Content moderation": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "people": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "behavioral harms": {
        "paragraphs": [
            "In the last lecture, we started discussing the harms (negative impacts) on people who use systems powered by large language models. We call these behavioral harms because these are harms due to the behavior of a language model rather than its construction (which would encompass data privacy and environmental impact).",
            "So far, we have described two types of behavioral harms:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Performance disparities": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social bias and stereotypes": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Benefits versus harms": {
        "paragraphs": [
            "Benefits versus harms. With any technology, it\u2019s important to consider the tradeoff between benefits and harms. This is very tricky business because:",
            "But it is important to study the harms of language models because:",
            "Upstream versus downstream."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "quantify": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "tradeoffs": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "legitimacy": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Upstream versus downstream": {
        "paragraphs": [
            "Upstream versus downstream.",
            "Benefits versus harms. With any technology, it\u2019s important to consider the tradeoff between benefits and harms. This is very tricky business because:",
            "In this lecture, we will discuss two more behavioral harms:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "downstream task": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "adapted": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "upstream language model": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "heavy lifting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "text": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "broader social context": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Context-dependence": {
        "paragraphs": [
            "Context-dependence. What constitutes harmful content is very context-dependent. Chandrasekhran et al. 2018 performed a detailed study on Reddit:",
            "Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation.",
            "While there are norms common to almost all subreddits, many norms are specific to subreddits, for example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "context-dependent": {
        "paragraphs": [
            "Context-dependence. What constitutes harmful content is very context-dependent. Chandrasekhran et al. 2018 performed a detailed study on Reddit:",
            "Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation.",
            "While there are norms common to almost all subreddits, many norms are specific to subreddits, for example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Dual use": {
        "paragraphs": [
            "Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "While there are norms common to almost all subreddits, many norms are specific to subreddits, for example:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generate": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "detect": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "user": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "recipient": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Working definition": {
        "paragraphs": [
            "Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition: Borkan et al, 2017 defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm:",
            "Word lists. How far can one get by simply defining toxicity in terms of presence of certain \u201cbad words\u201d?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Word lists": {
        "paragraphs": [
            "Word lists. How far can one get by simply defining toxicity in terms of presence of certain \u201cbad words\u201d?",
            "Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition: Borkan et al, 2017 defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples:",
            "Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "low agreement": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "warning: contains offensive content": {
        "paragraphs": [
            "For example (demo; warning: contains offensive content):",
            "Gehman et al, 2020 introduced a dataset to evaluate the toxicity of generation from a language model.",
            "Caveats."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Caveats": {
        "paragraphs": [
            "Caveats.",
            "For example (demo; warning: contains offensive content):",
            "Unprompted experiments."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Unprompted experiments": {
        "paragraphs": [
            "Unprompted experiments.",
            "Caveats.",
            "Prompting experiments."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Prompting experiments": {
        "paragraphs": [
            "Prompting experiments.",
            "Unprompted experiments.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Expected maximum toxicity": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Probability": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Mitigating toxicity": {
        "paragraphs": [
            "Mitigating toxicity.",
            "Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts.",
            "But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Mitigating": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Misinformation": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "intentionally": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "need not be falsifiable": {
        "paragraphs": [
            "Note that misinformation and disinformation need not be falsifiable; sometimes it incites or shifts burden of proof to the audience.",
            "Terminology (further discussion):",
            "Things that are not true, but don\u2019t count as misinformation or disinformation:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Fiction literature": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Satire": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "goal": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "manually": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "novel": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "fluent": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "persuasive": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "message": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "economics": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "humans in the loop": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "online radicalization": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Grover": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data behind large language models": {
        "paragraphs": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.",
            "It is worth noting that private datasets that reside in big companies are even larger than what\u2019s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.",
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Despite the size, large-scale data still has uneven representation over the population.Internet data overrepresents younger users from developed countries.GPT-2\u2019s training data is based on Reddit, which according to Pew Internet Research\u2019s 2016 survey, 67% of Reddit users in the US are men, 64% between ages 18 and 29.8.8-15% of Wikipedians are female.Harassment on Internet could turn away certain people (trans, queer, neurodivergent people).Filtering \u201cbad words\u201d could further marginalize certain populations (e.g., LGBT+)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "WebText and OpenWebText Data behind large language models": {
        "paragraphs": [
            "WebText. The WebText dataset was used to train GPT-2.",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.",
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Goal: obtain diverse but high-quality dataset.Previous work:Datasets were trained on news, Wikipedia, or fiction.Common Crawl contains a lot of junk (gibberish, boilerplate text).Trinh & Le, 2018 selected a tiny subset of Common Crawl based on n-gram overlap with the target task.Process for creating WebText:Scraped all outbound links that received at least 3 karma (upvotes).Filtered out Wikipedia to be able to evaluate on Wikipedia-based benchmarks.End result is 40 GB of text.",
            "Extracted all the URLs from the Reddit submissions dataset.Used Facebook\u2019s fastText to filter out non-English.Removed near duplicates.End result is 38 GB of text.",
            "2.1% of OpenWebText has toxicity score >= 50%4.3% of WebText (from OpenAI) has toxicity score >= 50%News reliability correlates negatively with toxicity (Spearman \\(\\rho = -0.35\\))3% of OpenWebText comes from banned or quarantined subreddits, e.g., /r/The_Donald and /r/WhiteRights"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Colossal Clean Crawled Corpus Data behind large language models": {
        "paragraphs": [
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.",
            "Analysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.",
            "Documentation levels:",
            "Note: Raffel et al. 2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.",
            "",
            "Benchmark data contamination.",
            "Example from the XSum summarization dataset:",
            "There are two types of contamination:",
            "Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).",
            "The dataset could also be responsible for various harms:",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Started with April 2019 snapshot of Common Crawl (1.4 trillion tokens)Removed \u201cbad words\u201dRemoved code (\u201c{\u201c)langdetect to filter out non-English textResulted in 806 GB of text (156 billion tokens)",
            "Metadata: provenance, utterance dataIncluded data: machine or human authored, social biases, data contaminationExcluded data: medical or health data, demographic identities",
            "A surprising amount of data from patents.google.com65% pages in the Internet Archive; out of those, 92% pages written in the last decade51.3% pages are hosted in the United States; fewer from India even though lots of English speakers thereSome text from patents.google.com are automatically created, and thus have systematic errors:Filed in a foreign country\u2019s official language (e.g., Japanese) is automatically translated into EnglishAutomatically generated from optical character recognition (OCR)",
            "When we are evaluating the capabilities of large language models using benchmark data (e.g., question-answer pairs), it makes a difference whether the benchmark data appears in the training data of the language model. If so, then the benchmark performance will be biased up.Normally, in machine learning, data hygiene (keeping the training data separate from the test) is relatively easy, but in the case of large language models, both the training data and benchmark data are derived from the Internet, it can be difficult to a priori guarantee their separation.",
            "Input-and-output contamination: both the input and output appear in the training data. Varies from 1.87% to 24.88% (XSum is 15.49%).Input contamination: the input appears in the training data. Varies from 1.8% to 53.6% (QNLI, which is derived from Wikipedia).",
            "Representational harmsThey look at co-occurrence with ethnicity terms (e.g., Jewish) and sentiment-bearing words (e.g., successful).Jewish has 73.2% positive sentiment, Arab has 65.7% positive (7.5% difference).Variation across sites (New York Times had a 4.5% difference, Al Jazeera had 0% difference).Allocational harmsRecall C4 is a filtered version of Common Crawl (only about 10%).Mentions of sexual orientations (e.g., lesbian, gay) more likely to be filtered out; of those filtered out, non-trivial fraction are non-offensive (e.g., 22% and 36%).Certain dialects are more likely to be filtered (AAE: 42%, Hispanic-aligned English: 32%) than others (White American English: 6.2%)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "GPT-3 dataset Data behind large language models": {
        "paragraphs": [
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Selected subset of Common Crawl that\u2019s similar to a reference dataset (WebText).Downloaded 41 shards of Common Crawl (2016-2019).Trained a binary classifier to predict WebText versus Common Crawl.Sampled (kept) a document with higher probability if classifier deems it more similar to WebText.Performed fuzzy deduplication (detect 13-gram overlap, remove window or documents if occurred in <10 training documents), removing data from benchmark datasets.Expanded the diversity of the data sources (WebText2, Books1, Books2, Wikipedia).During training, Common Crawl is downsampled (Common Crawl is 82% of the dataset, but contributes only 60%)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "The Pile Data behind large language models": {
        "paragraphs": [
            "Data composition.",
            "",
            "Compare:",
            "",
            "Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset.",
            "They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "While a web crawl is a natural place to look for broad data, it\u2019s not the only strategy, and GPT-3 already hinted that it might be productive to look at other sources of higher quality.EleutherAI (a nonprofit organization committed to building open language models), pushed this idea even farther.They released The Pile, a dataset for language modeling, where the key idea is to source it from smaller high-quality sources (academic + professional sources).",
            "825 GB English text22 high-quality datasets",
            "GPT-2Pile (1.5B parameters) trained on The PileGPT-3 (175B parameters) trained on GPT-3\u2019s dataset.Normalize so that the difference for OpenWebText2 is 0."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary Data behind large language models": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "The total amount of data out there (web, private data) is massive.Training on \u201call of it\u201d (even Common Crawl) doesn\u2019t work well (not effective use of compute).Filtering / curation (OpenWebText, C4, GPT-3 dataset) is needed, but can result in biases.Curating non-web high-quality datasets is promising (The Pile).Important to carefully document and inspect these datasets."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Documentation for datasets": {
        "paragraphs": [
            "We now step back from the specifics of language modeling datasets and discuss general principles around data.",
            "Two purposes:",
            "Dataset lifecycle (a sample of the questions from each category are provided below):",
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:",
            "As an example, let\u2019s look at the datasheet for The Pile."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "It has been long noted that documentation is important, but within the machine learning community, it has been a fairly ad-hoc process.Examples from other fields:Electronics industry has a well-established protocol where every component has a datasheet with operating characteristics, test results, recommended and usage.Nutrition labels: The FDA mandates that food be labeled with their nutrition content.Datasheets for datasets (Gebru et al., 2018) is an influential paper that provides community norms around documentation.Data statements (Bender & Friedman, 2018) is related framework that is more tailored to language datasets.The emphasis is on transparency.",
            "Dataset creators: reflect on decisions, potential harms (e.g., social biases) when creating the dataset.Dataset consumers: know when the dataset can and can\u2019t be used.",
            "MotivationFor what purpose was the dataset created?Who created this dataset?Who funded the creation of the dataset?CompositionWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Is any information missing from individual instances?Does the dataset contain data that might be considered confidential?Collection processHow was the data associated with each instance acquired?Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Were any ethical review processes conducted (e.g., by an institutional review board)?Preprocessing/cleaning/labelingWas any preprocessing/cleaning/labeling of the data done?Is the software that was used to preprocess/clean/label the data available?UsesHas the dataset been used for any tasks already?Are there tasks for which the dataset should not be used?DistributionHow will the dataset will be distributed?Have any third parties imposed IP-based or other restrictions on the data associated with the instances?MaintenanceWho will be supporting/hosting/maintaining the dataset?Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?",
            "Curation rationale (what\u2019s included?)Language variety (schema)Speaker demographic (age, gender, race/ethnicity, etc.)Annotator demographic (age, gender, race/ethnicity, etc.)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data ecosystems": {
        "paragraphs": [
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.",
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.",
            "",
            "Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Some issues are discussed in the data section of the foundation models report.Data governance talks about how an organization can create data, maintain its quality and security.The BigScience project (initiated by Hugging Face) aims to collect a large multilingual dataset as well as train a large language model. The BigScience data governance working group has been developing a framework to responsibly curate quality data sources, in contrast to the indiscriminate scraping of the web.",
            "People create data.Because people live in social environments, data also is a property not of individuals, but of groups of people. Examples: emails, genetic data.Individually, data does not have value, but collectively, it has a lot of value.Related: Data Shapley is a framework for ascribing value to a given data point in the context of machine learning.Status quo: people give away their data for free, and big corporations derive tons of value and power from it.Example: Alice and Bob are both writers. Alice provide examples of writing for free. This can be used to train a language model that can replace Bob.Think about data as labor rather than property rights.Data privacy works on the individual level, and doesn\u2019t work.Proposal: data coalitions, which are intermediate organizations that represent between data producers and data buyers (think about collective bargaining).Read this article for more details."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "training data": {
        "paragraphs": [
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:",
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.",
            "There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "broad": {
        "paragraphs": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.",
            "In the rest of the lecture, we\u2019ll talk about:",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "web": {
        "paragraphs": [
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.",
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.",
            "It is worth noting that private datasets that reside in big companies are even larger than what\u2019s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "private datasets": {
        "paragraphs": [
            "It is worth noting that private datasets that reside in big companies are even larger than what\u2019s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Common Crawl": {
        "paragraphs": [
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.",
            "It is worth noting that private datasets that reside in big companies are even larger than what\u2019s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!",
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Representation": {
        "paragraphs": [
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "uneven representation": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "WebText": {
        "paragraphs": [
            "WebText. The WebText dataset was used to train GPT-2.",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models.",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "diverse": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "high-quality": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "OpenWebText": {
        "paragraphs": [
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.",
            "WebText. The WebText dataset was used to train GPT-2.",
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Toxicity analysis": {
        "paragraphs": [
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.",
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Analysis": {
        "paragraphs": [
            "Analysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.",
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.",
            "Documentation levels:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Benchmark data contamination": {
        "paragraphs": [
            "Benchmark data contamination.",
            "",
            "Example from the XSum summarization dataset:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "biased": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input": {
        "paragraphs": [
            "Input: The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: \u201cHe played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012. Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Output": {
        "paragraphs": [
            "Input: The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: \u201cHe played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012. Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input-and-output contamination": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input contamination": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Representational harms": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Allocational harms": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "similar to a reference dataset": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "fuzzy deduplication": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data sources": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data composition": {
        "paragraphs": [
            "Data composition.",
            "",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Electronics industry": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Nutrition labels": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "transparency": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Dataset creators": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Dataset consumers": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Dataset lifecycle": {
        "paragraphs": [
            "Dataset lifecycle (a sample of the questions from each category are provided below):",
            "Two purposes:",
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data statements": {
        "paragraphs": [
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:",
            "Dataset lifecycle (a sample of the questions from each category are provided below):",
            "As an example, let\u2019s look at the datasheet for The Pile."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data management": {
        "paragraphs": [
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "industry": {
        "paragraphs": [
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data dignity": {
        "paragraphs": [
            "Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "",
            "Documentation for datasets:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "labor": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data coalitions": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Clossal Clean Crawled Corpus (C4)": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "CCNet": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "The Pile": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Copyright law": {
        "paragraphs": [
            "Copyright law. The key legislation that governs copyright in the United States is Copyright Act of 1976.",
            "Intellectual property law.",
            "There are two ways you can use a copyrighted work:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Privacy law": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Other laws": {
        "paragraphs": [
            "California\u2019s bot disclosure bill:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Illegal to use a bot to communicate with a person without disclosing that it\u2019s a botRestriction: applies only to incentivize a sale or influence a vote in an election.Restriction: applies only to public-facing websites with 10 million monthly US visitors."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Environmental impact is a huge topic. Everything is connected, so it\u2019s hard to get out a clean quantitative metric. But should really look at the whole picture.While the total footprint of large language models is small today, it is growing very quickly.General-purpose nature of large language models offers potential for savings (\u201ctrain once\u201d and apply to many different tasks). But they are much more expensive and need to be retrained. What are the tradeoffs?Mitigation:Try to train models on cleaner energy data centersCarbon offsets have varying effectiveness (forest planting campaigns yield monocultures)More efficient model architectures, training procedures, hardware (but beware of rebound effects)Reporting:Raises awareness (imagine if it was expected that every paper would report emissions)Aligning incentive (people currently fixate on accuracy, but carbon emissions is important too!)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "law": {
        "paragraphs": [
            "Non-legal considerations. There is a distinction between law and ethics.",
            "Jurisdiction. Depending on where you live (which country, which state, etc.), which laws apply vary."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "new powerful technology": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Internet law": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "jurisdiction": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "anonymous": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Anyone can post": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Non-legal considerations": {
        "paragraphs": [
            "Non-legal considerations. There is a distinction between law and ethics.",
            "Jurisdiction. Depending on where you live (which country, which state, etc.), which laws apply vary."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "ethics": {
        "paragraphs": [
            "Non-legal considerations. There is a distinction between law and ethics.",
            "Jurisdiction. Depending on where you live (which country, which state, etc.), which laws apply vary."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Jurisdiction": {
        "paragraphs": [
            "Jurisdiction. Depending on where you live (which country, which state, etc.), which laws apply vary.",
            "Non-legal considerations. There is a distinction between law and ethics.",
            "Types of law."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "countries": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "federal, state, or local": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Types of law": {
        "paragraphs": [
            "Types of law.",
            "Jurisdiction. Depending on where you live (which country, which state, etc.), which laws apply vary.",
            "Large language models. Now let turn our attention to large language models. Recall the lifecycle of a large language model:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Common law": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "precedent": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Statutory law": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Regulatory law": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Large language models": {
        "paragraphs": [
            "Large language models. On the other hand, we see a massive increase in the amount of compute required to train large language models (and therefore contributing to emissions). Here are some example data points:",
            "Climate change. On one hand, we\u2019ve all heard about the very serious dangers of climate change (article, article):",
            "How do connect large language models and environmental impact?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "lifecycle": {
        "paragraphs": [
            "Large language models. Now let turn our attention to large language models. Recall the lifecycle of a large language model:",
            "Types of law.",
            "There are two main areas where the law intersects the large language models lifecycle:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Applications": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "high-stakes": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Intellectual property law": {
        "paragraphs": [
            "Intellectual property law.",
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law.",
            "Copyright law. The key legislation that governs copyright in the United States is Copyright Act of 1976."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "fixed": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "expression": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Registration is not required": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "public domain": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Licenses": {
        "paragraphs": [
            "Licenses.",
            "There are two ways you can use a copyrighted work:",
            "Fair use (section 107)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "license": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Fair use (section 107)": {
        "paragraphs": [
            "Fair use (section 107).",
            "Licenses.",
            "Terms of service. There is one additional hurdle: terms of service, which might impose additional restrictions."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "purpose": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "transformative": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "original work": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "market": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Terms of service": {
        "paragraphs": [
            "Terms of service. There is one additional hurdle: terms of service, which might impose additional restrictions.",
            "Fair use (section 107).",
            "Notes:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Notes": {
        "paragraphs": [
            "Notes:",
            "Terms of service. There is one additional hurdle: terms of service, which might impose additional restrictions.",
            "Next, we will go over a number of cases that have ruled for or against fair use."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "fair use": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "not fair use": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "idea": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generative models": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "compete": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "uncurated": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generative": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Tokenization": {
        "paragraphs": [
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):",
            "However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):",
            "A tokenizer converts any string into a sequence of tokens.",
            "This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Split by spaces Tokenization": {
        "paragraphs": [
            "The simplest solution is to do:",
            "Therefore, splitting by spaces by spaces to identify words is quite problematic.",
            "What makes a good tokenization?"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "This doesn\u2019t work for languages such as Chinese, where sentences are written without spaces between words:",
            "Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don\u2019t), which should get split up. For example, the Penn Treebank splits don\u2019t into do and n\u2019t, a linguistically-informed but not obvious choice.",
            "We don\u2019t want too many tokens (extreme: characters or bytes), or else the sequence becomes difficult to model.We don\u2019t want too few tokens, or else there won\u2019t be parameter sharing between words (e.g., should mother-in-law and father-in-law be completely different)? This is especially problematic for morphologically rich languages (e.g., Arabic, Turkish, etc.).Each token should be a linguistically or statistically meaningful unit."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Byte pair encoding Tokenization": {
        "paragraphs": [
            "Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.",
            "Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.",
            "Example:",
            "The output of learning is:",
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:",
            "Unicode."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Input: a training corpus (sequence of characters).Initialize the vocabulary \\(\\sV\\) be the set of characters.While we want to still grow \\(\\sV\\):Find the pair of elements \\(x,x' \\in \\sV\\) that co-occur the most number of times.Replace all occurrences of \\(x, x'\\) with a new symbol \\(x x'\\).Add \\(x x'\\) to \\(\\sV\\).",
            "[t, h, e, \u2423, c, a, r], [t, h, e, \u2423, c, a, t], [t, h, e, \u2423, r, a, t][th, e, \u2423, c, a, r], [th, e, \u2423, c, a, t], [th, e, \u2423, r, a, t] (th occurs 3x)[the, \u2423, c, a, r], [the, \u2423, c, a, t], [the, \u2423, r, a, t] (the occurs 3x)[the, \u2423, ca, r], [the, \u2423, ca, t], [the, \u2423, r, a, t] (ca occurs 2x)",
            "Updated vocabulary \\(\\sV\\): [a, c, e, h, t, r, ca, th, the]The merges that we made (important for applying the tokenizer):t, h \\(\\Rightarrow\\) thth, e \\(\\Rightarrow\\) thec, a \\(\\Rightarrow\\) ca",
            "[t, h, e, \u2423, o, x][th, e, \u2423, o, x][the, \u2423, o, x]",
            "One problem is that (especially in the multilingual setting), there are a lot (144,697) of Unicode characters.We certainly will not see all characters in the training data.In order to reduce data sparsity even further, we can run BPE on bytes instead of Unicode characters (Wang et al. 2019).Example in Chinese:"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Unigram model (SentencePiece) Tokenization": {
        "paragraphs": [
            "Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).",
            "Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of",
            "Example:",
            "Algorithm:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "It was of the tokenizations supported in the SentencePiece tool (Kudo & Richardson, 2018), along with BPE.It was used to train T5 and Gopher.",
            "Training data (string): \\(\\nl{ababc}\\)Tokenization \\(T = \\{ (1, 2), (3, 4), (5, 5) \\}\\) (\\(\\sV = \\{ \\nl{ab}, \\nl{c} \\}\\))Likelihood: \\(p(x_{1:L}) = \\frac{2}{3} \\cdot \\frac{2}{3} \\cdot \\frac{1}{3} = \\frac{4}{9}\\).",
            "Start with a \u201creasonably big\u201d seed vocabulary \\(\\sV\\).Repeat:Given \\(\\sV\\), optimize \\(p(x)\\) and \\(T\\) using the EM algorithm.Compute \\(\\text{loss}(x)\\) for each token \\(x \\in \\sV\\) capturing how much the likelihood would be reduced if \\(x\\) were removed from \\(\\sV\\).Sort by loss and keep the top 80% tokens in \\(\\sV\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Comparing tokenizers Tokenization": {
        "paragraphs": [
            "Impact:",
            "Examples of tokenizations for both GPT-3 and Jurassic (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "GPT-2 and GPT-3 used BPE, vocabulary size of 50KJurassic used SentencePiece with vocabulary size of 256K",
            "Given the same string, Jurassic requires 28% fewer tokens than GPT-3, so it is 1.4x fasterBoth Jurassic and GPT-3 use the same context size (2048), so one can feed in 39% more text into the prompt.",
            "GPT-3: [Ab, raham, \u2423Lincoln, \u2423lived, \u2423at, \u2423the, \u2423White, \u2423House, .]Jurassic: [Abraham\u2423Lincoln, \u2423lived, \u2423at\u2423the\u2423White\u2423House, .]"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Models": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Types of language models Models": {
        "paragraphs": [
            "We will broaden our notion of language models to three types of models.",
            "Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).",
            "Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)).",
            "Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Example: sentiment classification",
            "Example: natural language inference",
            "Pro: contextual embedding for \\(x_i\\) can depend bidirectionally on both the left context (\\(x_{1:i-1}\\)) and the right context (\\(x_{i+1:L}\\)).Con: cannot naturally generate completions.Con: requires more ad-hoc training objectives (masked language modeling).",
            "Example: text autocomplete",
            "Con: contextual embedding for \\(x_i\\) can only depend unidirectionally on both the left context (\\(x_{1:i-1}\\)).Pro: can naturally generate completions.Pro: simple training objective (maximum likelihood).",
            "Example: table-to-text generation",
            "Pro: contextual embedding for \\(x_i\\) can depend bidirectionally on both the left context (\\(x_{1:i-1}\\)) and the right context (\\(x_{i+1:L}\\)).Pro: can naturally generate outputs.Con: requires more ad-hoc training objectives."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Preliminaries Models": {
        "paragraphs": [
            "First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data):",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):",
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Turns each token \\(x_i\\) in the sequence \\(\\x\\) into a vector.Return \\([E_{x_1}, \\dots, E_{x_L}]\\).",
            "Process each element \\(x_i\\) in the sequence \\(\\x\\) with respect to other elements.[abstract implementation (e.g., \\(\\FeedForwardSequenceModel\\), \\(\\SequenceRNN\\), \\(\\TransformerBlock\\))]",
            "Process each element \\(x_i\\) in the sequence \\(\\x\\) by looking at the last \\(n\\) elements..For each \\(i = 1, \\dots, L\\):Compute \\(h_i = \\FeedForward(x_{i-n+1}, \\dots, x_i)\\).Return \\([h_1, \\dots, h_L]\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Recurrent neural networks Models": {
        "paragraphs": [
            "The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.",
            "def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state:",
            "def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):",
            "There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)).",
            "def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):",
            "As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used by ELMo and ULMFiT.",
            "def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\):",
            "Notes:",
            "We will not discuss these models in the interest of time."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Process the sequence \\(x_1, \\dots, x_L\\) left-to-right and recursively compute vectors \\(h_1, \\dots, h_L\\).For \\(i = 1, \\dots, L\\):Compute \\(h_i = \\RNN(h_{i-1}, x_i)\\).Return \\([h_1, \\dots, h_L]\\).",
            "Updates the hidden state \\(h\\) based on a new observation \\(x\\).[abstract implementation (e.g., \\(\\SimpleRNN\\), \\(\\LSTM\\), \\(\\GRU\\))]",
            "Updates the hidden state \\(h\\) based on a new observation \\(x\\) by simple linear transformation and non-linearity.Return \\(\\sigma(U h + V x + b)\\).",
            "Process the sequence both left-to-right and right-to-left.Compute left-to-right: \\([h_1^\\rightarrow, \\dots, \\vec{h}_L^\\rightarrow] \\leftarrow \\SequenceRNN(x_1, \\dots, x_L)\\).Compute right-to-left: \\([h_L^\\leftarrow, \\dots, h_1^\\leftarrow] \\leftarrow \\SequenceRNN(x_L, \\dots, x_1)\\).Return \\([h_1^\\rightarrow h_1^\\leftarrow, \\dots, h_L^\\rightarrow h_L^\\leftarrow]\\).",
            "The simple RNN is difficult to train due to vanishing gradients.The Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) (both of \\(\\RNN\\)) have been developed to address these.Still, even though the embedding \\(h_{200}\\) can depend arbitrarily far back (e.g., on \\(x_1\\)), it is unlikely to depend on it in a \u201ccrisp\u201d way (see Khandelwal et al., 2018 for more discussion).LSTMs in some sense were really what brought deep learning into full swing within NLP."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Transformers Models": {
        "paragraphs": [
            "Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.",
            "There are great resources for learning about the Transformer:",
            "You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.",
            "The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).",
            "One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\):",
            "We can think of each \\(x_i\\) as representing a key-value pair via linear transformations:",
            "and forming the query via another linear transformation:",
            "The key and the query can be compared to give a score:",
            "These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\):",
            "Then the final output is a weighted combination over the values:",
            "We can write this all succinctly in matrix form:",
            "def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\):",
            "We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)",
            "Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce:",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):",
            "Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide:",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.",
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):",
            "we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\):",
            "Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:",
            "def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d:",
            "def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "Finally, we can define the Transformer block succinctly as follows:",
            "def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.",
            "To fix this, we add positional information into the embedding:",
            "def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\):",
            "GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:",
            "Shape of the architecture (how the 175 billion parameters are allocated):",
            "These decisions are not necessarily optimal. Levine et al. 2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.",
            "There are important but detailed differences between different versions of Transformers:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Illustrated Transformer and Illustrated GPT-2: very nice visual description of the Transformer.Annotated Transformer: Pytorch implementation of the Transformer.",
            "Process \\(y\\) by comparing it to each \\(x_i\\).Return \\(W_\\text{value} \\, \\x \\, \\softmax(\\x^\\top W_\\text{key}^\\top W_\\text{query} y / \\sqrt{d})\\).",
            "Process \\(y\\) by comparing it to each \\(x_i\\) with respect to \\(n_\\text{heads}\\) aspects.Return \\(W_\\text{output} \\underbrace{[\\Attention(\\x, y), \\dots, \\Attention(\\x, y)]}_{n_\\text{heads} \\text{times}}\\).",
            "Compare each element \\(x_i\\) to each other element.Return \\([\\Attention(\\x, x_1), \\dots, \\Attention(\\x, x_L)]\\).",
            "Process each token independently.For \\(i = 1, \\dots, L\\):Compute \\(y_i = W_2 \\max(W_1 x_i + b_1, 0) + b_2\\).Return \\([y_1, \\dots, y_L]\\).",
            "Make each \\(x_i\\) not too big or small.",
            "Safely apply \\(f\\) to \\(\\x\\).Return \\(\\LayerNorm(\\x + f(\\x))\\).",
            "Process each element \\(x_i\\) in context.Return \\(\\AddNorm(\\FeedForward, \\AddNorm(\\SelfAttention, \\x))\\).",
            "Add in positional information.Define positional embeddings:Even dimensions: \\(P_{i,2j} = \\sin(i / 10000^{2j/d_\\text{model}})\\)Odd dimensions: \\(P_{i,2j+1} = \\cos(i / 10000^{2j/d_\\text{model}})\\)Return \\([x_1 + P_1, \\dots, x_L + P_L]\\).",
            "Dimension of hidden state: \\(d_\\text{model} = 12288\\)Dimension of the intermediate feed-forward layer: \\(d_\\text{ff} = 4 d_\\text{model}\\)Number of heads: \\(n_\\text{heads} = 96\\)Context length: \\(L = 2048\\)",
            "Layer normalization \u201cpost-norm\u201d (original Transformers paper) versus pre-norm (GPT-2), which impacts training stability (Davis et al. 2021).Dropout is applied throughout to prevent overfitting.GPT-3 uses a sparse Transformer to reduce the number of parameters, interleaving it with dense layers.Depending on the type of Transformer (encoder-only, decoder-only, encoder-decoder), different masking operations are used.And of course there are many more details involved in the training of Transformer models which we will discuss next time."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "tokenization": {
        "paragraphs": [
            "Today\u2019s lecture will focus on two topics, tokenization and model architecture.",
            "In this lecture, we will open up the onion all the way and talk about how large language models are built.",
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "model architecture": {
        "paragraphs": [
            "Today\u2019s lecture will focus on two topics, tokenization and model architecture.",
            "In this lecture, we will open up the onion all the way and talk about how large language models are built.",
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "sequence of tokens": {
        "paragraphs": [
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):",
            "Today\u2019s lecture will focus on two topics, tokenization and model architecture.",
            "However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "tokenizer": {
        "paragraphs": [
            "A tokenizer converts any string into a sequence of tokens.",
            "However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):",
            "This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "many": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "few": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Learning the tokenizer": {
        "paragraphs": [
            "Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.",
            "Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.",
            "Example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Applying the tokenizer": {
        "paragraphs": [
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:",
            "The output of learning is:",
            "Unicode."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Unicode": {
        "paragraphs": [
            "Unicode.",
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:",
            "Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "unigram model": {
        "paragraphs": [
            "Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).",
            "Unicode.",
            "Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "SentencePiece": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Algorithm": {
        "paragraphs": [
            "Algorithm:",
            "Example:",
            "Impact:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Contextual embeddings": {
        "paragraphs": [
            "Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.",
            "We will broaden our notion of language models to three types of models."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Encoder-only": {
        "paragraphs": [
            "Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.",
            "We will broaden our notion of language models to three types of models.",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "bidirectionally": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "ad-hoc training": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Decoder-only": {
        "paragraphs": [
            "Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)).",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).",
            "Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "unidirectionally": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "simple training": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Encoder-decoder": {
        "paragraphs": [
            "Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework:",
            "We now turn to another class of language models, retrieval-based (or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer.",
            "Example (open-book question answering):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "context-independent embeddings": {
        "paragraphs": [
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "contextual embeddings": {
        "paragraphs": [
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "fixed length": {
        "paragraphs": [
            "The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "hidden states": {
        "paragraphs": [
            "The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "attention mechanism": {
        "paragraphs": [
            "The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).",
            "You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.",
            "One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "attention heads": {
        "paragraphs": [
            "We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.",
            "def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\):",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Self-attention layer": {
        "paragraphs": [
            "Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce:",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Feedforward layer": {
        "paragraphs": [
            "Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide:",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Improving trainability": {
        "paragraphs": [
            "Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Residual connections": {
        "paragraphs": [
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):",
            "Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.",
            "we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Layer normalization": {
        "paragraphs": [
            "Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:",
            "we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\):",
            "def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Positional embeddings": {
        "paragraphs": [
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.",
            "def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):",
            "To fix this, we add positional information into the embedding:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "positional information": {
        "paragraphs": [
            "To fix this, we add positional information into the embedding:",
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.",
            "def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "GPT-3": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "byte pair encoding": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "WordPiece": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Alibi embeddings": {
        "paragraphs": [
            "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. Ofir Press, Noah A. Smith, M. Lewis. 2021. Introduces Alibi embeddings."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Sparse Transformers": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Linformers": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Performers": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "GPT-2": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Gopher": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Jurassic": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "BERT": {
        "paragraphs": [
            "BERT. We will first present the BERT objective function, which contains two terms:",
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate.",
            "Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "RoBERTa": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "BART": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "T5": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Objective functions": {
        "paragraphs": [
            "We will consider objective functions for the three types of language models:",
            "We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Decoder-only (e.g., GPT-3): compute unidirectional contextual embeddings, generate one token at a timeEncoder-only (e.g., BERT): compute bidirectional contextual embeddingsEncoder-decoder (e.g., T5): encode input, decode output"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Decoder-only models Objective functions": {
        "paragraphs": [
            "Recall that an autoregressive language model defines a conditional distribution:",
            "We define it as follows:",
            "Succinctly:",
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.",
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:",
            "There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Map \\(x_{1:i-1}\\) to contextual embeddings \\(\\phi(x_{1:i-1})\\).Apply an embedding matrix \\(E \\in \\R^{V \\times d}\\) to obtain scores for each token \\(E \\phi(x_{1:i-1})_{i-1}\\).Exponentiate and normalize it to produce the distribution over \\(x_i\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Encoder-only models Objective functions": {
        "paragraphs": [
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate.",
            "BERT. We will first present the BERT objective function, which contains two terms:",
            "Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):",
            "There are two special tokens:",
            "Using our notation from the previous lecture, the BERT model is defined as:",
            "where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:",
            "",
            "BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters.",
            "Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).",
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:",
            "Here\u2019s how \\(A\\) is defined:",
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:",
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\).",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\).",
            "We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision.",
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:",
            "Objective. Then the BERT objective is:",
            "We will talk about training later, but a few quick notes about BERT:",
            "RoBERTa makes the following changes to BERT:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Masked language modelingNext sentence prediction",
            "\\(\\CLS\\): contains the embedding used to drive classification tasks\\(\\SEP\\): used to tell the model where the first (e.g., premise) versus second sequence (e.g., hypothesis) are.",
            "\\(e_A \\in \\R^d\\) for tokens left of \\(\\SEP\\), and\\(e_B \\in \\R^d\\) for tokens right of \\(\\SEP\\).",
            "Let \\(I \\subset \\{1, \\dots, L\\}\\) be a random 15% of the tokens positions.For each \\(i \\in I\\):With probability 0.8, set \\(\\tilde x_i \\leftarrow \\MASK\\).With probability 0.1, set \\(\\tilde x_i \\leftarrow x_i\\).With probability 0.1, set \\(\\tilde x_i \\leftarrow \\text{random word from } \\sV\\).",
            "During training, every input BERT would only see sequences with a \\(\\MASK\\).At test time, we would feed in sentences with no \\(\\MASK\\), resulting in a distribution shift. The heuristic fix is to replace with real words 20% of the time.",
            "Let \\(A\\) be a sentence from the corpus.With probability 0.5, let \\(B\\) be the next sentence.With probability 0.5, let \\(B\\) be a random sentence from the corpus.Let \\(\\x = [\\CLS, A, \\SEP, B]\\).Let \\(c\\) denote whether \\(B\\) is the next sentence or not.",
            "BERT (along with ELMo and ULMFiT) showed that one uniform architecture (Transformer) could be used for many multiple classification tasks.BERT really transformed the NLP community into a pre-training + fine-tuning mindset.BERT showed the importance of having deeply bidirectional contextual embeddings, although it\u2019s possible that model size and fine-tuning strategies make up for it (p-tuning).",
            "Removed the next sentence prediction objective (found it didn\u2019t help).Trained on more data (16GB text \\(\\rightarrow\\) 160GB text).Trained for longer. RoBERTa improved accuracy significantly over BERT on various benchmarks (e.g., on SQuAD 81.8 to 89.4)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Encoder-decoder models Objective functions": {
        "paragraphs": [
            "Example task (table-to-text generation):",
            "Recall that encoder-decoder models (e.g., BART, T5):",
            "BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.",
            "BART considers the following transformations \\(A(\\tx \\mid \\x)\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:",
            "They demonstrated strong results on both classification and generation tasks using fine-tuning.",
            "T5 (Text-to-Text Transfer Transformer).",
            "T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.",
            "Tasks:",
            "This paper experimented with many different unsupervised objectives:  and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar).",
            "They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:  Note the difference in approach to classification tasks:",
            "Notes:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Encode the input bidirectionally like BERT.Decode the output autoregressively like GPT-2.",
            "Same encoder architecture as RoBERTa (12 layers, hidden dimension 1024).Trained on same data as RoBERTa (160GB text).",
            "Mask 30% of tokens in a documentPermute all sentences",
            "Given a span of text, split at random point into input and output:",
            "BERT used the embedding of the \\(\\CLS\\) token to predict.T5, GPT-2, GPT-3, etc. (models that can generate) cast the classification tasks in a natural language space.",
            "The paper does a thorough study of many aspects of the entire pipeline (dataset, model size, training objective, etc.).Based on the insights, they trained a 11B parameter model."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Optimization algorithms": {
        "paragraphs": [
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling:",
            "Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:",
            "The key concerns in optimization are:",
            "There are several levels that we can approach optimization:",
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:",
            "Updating parameters.",
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)).",
            "AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.",
            "Mixed-precision training is another method for reducing memory (Narang et al., 2018).",
            "",
            "Learning rates.",
            "Initialization.",
            "For GPT-3:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Initialize parameters \\(\\theta_0\\).Repeat:Sample a mini-batch \\(B_t \\subset \\sD\\).Perform a gradient step:",
            "We want \\(\\theta\\) to converge quickly to a good solution.We want the optimization to be numerically stable.We want to be memory efficient (especially for large models). These are often at odds with each other (e.g., fast convergence and cutting down on memory by low-precision produces less stable training).",
            "Classic optimization: second-order methods, constrained optimization, etc.Machine learning: stochastic methods, implicit regularization + early stoppingDeep learning: initialization, normalization (changes to the model architecture)Large language models: stability issues, weird learning rates While some of the intuitions (e.g., second-order methods) are still useful, there are many other unique challenges that need to be overcome for large language model training to work. Unfortunately, much of this is fairly ad-hoc and poorly understood.",
            "Use momentum (keep on moving in the same direction).Have an adaptive (different) step size for each dimension of \\(\\theta\\) (inspiration from second-order methods).Initialize parameters \\(\\theta_0\\).Initialize moments \\(m_0, v_0 \\leftarrow 0\\).Repeat:Sample a mini-batch \\(B_t \\subset \\sD\\).Update parameters as follows.",
            "Compute gradient:",
            "Update first- and second-order moments:",
            "Do bias correction:",
            "Update parameters:",
            "Instead of storing the moments (\\(m_t,v_t\\)) of a \\(O(m \\times n)\\) matrix, store row and column sums (\\(O(m + n)\\) memory) and reconstruct the matrix.Remove momentum.It was used to train T5.It can be difficult to get AdaFactor to train (see Twitter thread and blog post).",
            "Default: FP32 (32-bit floating point).Option: FP16 (16-bit floating point), but the problem is that any value less than \\(2^{-24}\\) becomes 0.Solution: store master weights in FP32 and do everything else in FP16.Loss scaling: scale up loss to avoid gradients with small magnitudes.Result: Halves the memory usage.",
            "Normally, the learning rate decreases over time.For Transformers, we actually need to increase the learning rate (warmup).Huang et al., 2020 show that a potential reason for this is to prevent vanishing gradients from layer normalization leads to instability in Adam optimizer.",
            "Given a matrix \\(W \\in \\R^{m \\times n}\\), the standard initialization (xavier initialization) is \\(W_{ij} \\sim \\sN(0, 1/n)\\), where \\(n\\) is the fan-in.GPT-2 and GPT-3 scale the weights by an additional \\(1/\\sqrt{N}\\), where \\(N\\) is the number of residual layers.T5 scales the attention matrices by an additional \\(1/\\sqrt{d}\\) (code).",
            "Adam parameters: \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.95\\), \\(\\epsilon = 10^{-8}\\).Batch size: 3.2 million tokens (~1500 sequences)Use gradient clipping (\\(g_t \\leftarrow g_t / \\min(1, \\|g\\|_2)\\)).Linear learning rate warmup (over first 375 million tokens).Cosine learning rate that goes down to 10% of value.Gradually increase the batch size.Weight decay 0.1."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Maximum likelihood": {
        "paragraphs": [
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.",
            "Succinctly:",
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Unidirectional to bidirectional": {
        "paragraphs": [
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate.",
            "There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective.",
            "BERT. We will first present the BERT objective function, which contains two terms:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "left": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "right": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Masked language modeling": {
        "paragraphs": [
            "Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:",
            "BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters.",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Model": {
        "paragraphs": [
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "independently": {
        "paragraphs": [
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Masking function": {
        "paragraphs": [
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:",
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:",
            "Here\u2019s how \\(A\\) is defined:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Reducing distribution shift": {
        "paragraphs": [
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:",
            "Here\u2019s how \\(A\\) is defined:",
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Next sentence prediction": {
        "paragraphs": [
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.",
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Dataset": {
        "paragraphs": [
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:",
            "We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision.",
            "Objective. Then the BERT objective is:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Objective": {
        "paragraphs": [
            "Objective. Then the BERT objective is:",
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:",
            "We will talk about training later, but a few quick notes about BERT:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "BART (Bidirectional Auto-Regressive Transformers)": {
        "paragraphs": [
            "BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.",
            "Recall that encoder-decoder models (e.g., BART, T5):",
            "BART considers the following transformations \\(A(\\tx \\mid \\x)\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "T5 (Text-to-Text Transfer Transformer)": {
        "paragraphs": [
            "T5 (Text-to-Text Transfer Transformer).",
            "They demonstrated strong results on both classification and generation tasks using fine-tuning.",
            "T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "natural": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "study": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "11B parameter": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Stochastic gradient descent (SGD)": {
        "paragraphs": [
            "Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling:",
            "The key concerns in optimization are:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "quickly": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "stable": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "memory efficient": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "ADAM (adaptive moment estimation)": {
        "paragraphs": [
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:",
            "There are several levels that we can approach optimization:",
            "Updating parameters."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "momentum": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "adaptive": {
        "paragraphs": [
            "Have an adaptive (different) step size for each dimension of \\(\\theta\\) (inspiration from second-order methods)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Updating parameters": {
        "paragraphs": [
            "Updating parameters.",
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:",
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Memory": {
        "paragraphs": [
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)).",
            "Updating parameters.",
            "AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Learning rates": {
        "paragraphs": [
            "Learning rates.",
            "",
            "Initialization."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "decreases": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "increase": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Initialization": {
        "paragraphs": [
            "Initialization.",
            "Learning rates.",
            "For GPT-3:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "AdamW": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Mixture of experts": {
        "paragraphs": [
            "Basics. The idea of mixture of experts goes back to Jacobs et al. (1991).",
            "",
            "To introduce the basic idea, suppose we are solving a prediction problem:",
            "Let us start out by learning a feedforward (ReLU) neural network:",
            "where the parameters are \\(\\theta = (W_1, W_2)\\).",
            "But the mixture-of-experts approach is to:",
            "Example. Consider \\(d = 2\\) and each expert being a linear classifier (source):",
            "Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields:",
            "Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts.",
            "Saving compute.",
            "Balancing experts.",
            "Parallelism."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "However, this function might not be powerful enough to represent the function of interest.We could make the neural network wider or deeper.",
            "Define \\(E\\) experts.Each expert \\(e = 1, \\dots, E\\) has an embedding \\(w_e \\in \\R^d\\).Define the gating function as a probability distribution over the \\(E\\) experts:",
            "Each expert \\(e = 1, \\dots, E\\) has parameters \\(\\theta^{(e)} = (W_1^{(e)}, W_2^{(e)})\\).Define each expert function in terms of the expert-specific parameters:",
            "Define the final function as a mixture of the experts:",
            "Note the gating function \\(g(x) = [g_1(x), \\dots, g_E(x)]\\) is non-zero for each expert. For example:",
            "As written, the mixture of experts doesn\u2019t save any compute, because a feedforward pass would still have to evaluate each expert, and the backward pass would also have to touch each expert.However, if we approximate the gating function \\(g(x) = [g_1(x), \\dots, g_E(x)]\\) with \\(\\tilde g(x) = [\\tilde g_1(x), \\dots, \\tilde g_E(x)]\\) which places zero on most experts, then in the forward pass, we only have to evaluate the experts \\(e\\) with nonzero \\(\\tilde g_e(x)\\) (for both the forward and the backward pass).For example, we might take top 2 experts and renormalize:",
            "Mixture of experts is only effective if all experts pitch in.If only one expert is active (e.g., \\(g(x) = [0, 1, 0, 0]\\)), then this is a waste.Furthermore, if we end up in this state, then the gradients for the unused experts will be zero, and therefore they will not receive any gradients and improve.Therefore, one of the main considerations in using mixture-of-experts is to ensure that all the experts are used across inputs.",
            "The mixture-of-experts is very conducive to parallelization.Each expert can occupy a different machine.We compute the approximate gating function \\(\\tilde g(x)\\) centrally.Then we ask only the (sparse) set of machines containing activated experts to process \\(x\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Sparsely-gated mixture of experts (Lepikhin et al. 2021) Mixture of experts": {
        "paragraphs": [
            "",
            "We define the top-2 experts approximate gating function as follows:",
            "Notation:",
            "Balancing experts.",
            "For example, we can take \\(\\lambda = \\frac{0.01}{B}\\).",
            "Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts:",
            "The counter would be:",
            "We would try to push down on the gating function on expert 2 to discourage its use."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "We now consider how the mixture-of-experts idea can be applied to language modeling.The naive solution would be to have a mixture of 96-layer Transformers, butthe gating function would need to somehow need to apply to a sequence; andthe combination of experts only happens superficially at the top.Therefore, we will apply the mixture-of-experts idea to:each token andeach Transformer block (or every other one).Since the feed-forward layer is independent for each token, we turn each feed-forward network into a mixture-of-experts (MoE) feed-forward network:",
            "Every other Transformer block uses a MoE Transformer block.",
            "Compute the top expert: \\(e_1 = \\arg\\max_e g_e(x)\\).Compute the second expert: \\(e_2 = \\arg\\max_{e \\neq e_1} g_e(x)\\).Always keep top expert and keep the second expert stochastically:Let \\(p = \\min(2 g_{e_2}(x), 1)\\).With probability \\(p\\), set \\(\\tilde g_{e_1}(x) = \\frac{g_{e_1}(x)}{g_{e_1}(x) + g_{e_2}(x)}\\), \\(\\tilde g_{e_2}(x) = \\frac{g_{e_2}(x)}{g_{e_1}(x) + g_{e_2}(x)}\\), \\(\\tilde g_e(x) = 0\\) for \\(e \\not\\in \\{ e_1, e_2 \\}\\).With probability \\(1 - p\\): \\(\\tilde g_{e_1}(x) = 1\\), and \\(\\tilde g_e(x) = 0\\) for \\(e \\neq e_1\\).",
            "Let \\(B\\) be the number of tokens in the batch (across all sequences); usually on the order of millions.Let \\(E\\) be the number of experts; usually on the order of thousands.Let \\(x_1, \\dots, x_B\\) be the tokens in the batch.",
            "Let \\(c_e = \\sum_{i=1}^B \\mathbf{1}[\\tilde g_e(x_i) > 0]\\) be the number of times expert \\(e\\) is selected.Note that after processing a batch, \\(\\sum_e c_e = B\\).If all the experts were balanced, then \\(c_e = \\frac{B}{E}\\).Overflow: If \\(c_e > 2 \\frac{B}{E}\\), then set \\(f(x) = x\\) (bypass with residual connection), where \\(2\\) here is the capacity factor.Auxiliary loss: We would like to encourage \\(c = [c_1, \\dots, c_E]\\) to close to uniform.We could penalize \\(\\|c\\|_2^2 = \\sum_{e=1}^E c_e^2\\), but this is not differentiable.Define \\(m_e = \\sum_{i = 1}^B g_e(x_i)\\) (this is the soft version of \\(c_e\\)).Instead, we add \\(\\text{load-balancing-loss} = \\sum_{e=1}^E m_e c_e\\) to the objective function. This way, the gradient will be nonzero through \\(m_e\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Switch Transformer (Fedus et al. 2021) Mixture of experts": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Defines the approximate gating function \\(\\tilde g(x)\\) to only be the top-1 expert (to get even more sparsity).Tricks:Does selective casting from FP32 to FP16Smaller parameters for initializationExpert dropoutExpert parallelismTrained a 1.6 trillion parameter modelImproved pre-training speed compared to T5-XXL (11 billion parameters) by 4x"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Balanced Assignment of Sparse Experts (BASE) layers (Lewis et al., 2021) Mixture of experts": {
        "paragraphs": [
            "Experimental setup:",
            "",
            "BASE requires more compute to optimize the assignment \\(a\\), but is more stable.",
            "Summary and next steps."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "BASE defines the approximate gating function \\(\\tilde g(x)\\) to be the result of a joint optimization over all the tokens in the batch.We will assign each token 1 expert, but load balancing is a constraint rather than a soft penalty.We define \\(a = [a_1, \\dots, a_B] \\in \\{1, \\dots, E\\}^B\\) to be the joint assignment vector.",
            "This is a linear program that can be solved efficiently.In practice, we parallelize the linear program.At test time, just choose the top-1 expert.",
            "Sparsely gated MoE (top-2 experts): 52.5B parametersSwitch Transformer (top-1 expert): 52.5B parametersBASE (1 jointly optimized expert): 44.4B parameters (1.3B shared parameters, 335M x 128 expert parameters)",
            "Switch Transformer (Google) used top-1 expert.BASE (Facebook) used 1 expert per token, but jointly optimized.Neither of these competed with GPT-3. Since then, both Google and Facebook released two most recent high-performing MoE language models that do compete with GPT-3, but interestingly, they are still based on the original simple top-2 experts:GLaM from Google\u201cFacebookMoE\u201d from Facebook"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Generalist Language Model (GLaM) (Du et al. 2021) Mixture of experts": {
        "paragraphs": [
            "Specification:",
            "Other upgrades:",
            "Results:",
            "",
            "Results on WinoGender:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "1.2 trillion parameters (GPT-3 had 175 billion parameters)64 experts (not that many), 64 layers, 32K hidden unitsEach token activates 95B (8% of 1.2T) of the parameters",
            "Created new dataset (GLaM dataset) of 1.6 trillion tokens of webpages, forums, books, news, etc.Relative positional embeddings, Gated linear units, GeLU activation function, RMSNorm (not LayerNorm)Skip weight updates / rollback to earlier checkpoint if encounter NaN/Inf.\u201cWith the above tricks carefully implemented, we observe that the training of sparsely activated models at all scales becomes quite stable.\u201d",
            "1/3 of the cost to train compared to GPT-3Evaluated on same benchmarks as GPT-3 (open-domain question answering, reading comprehension, SuperGLUE, etc.)Achieved better 0-shot and 1-shot performance compared to GPT-3 (especially performant on knowledge-intensive tasks)Note: they did not evaluate in the few-shot, where GPT-3 is stronger",
            "Example: The nurse notified the patient that {her/his,their} shift would be ending in an hour.GPT-3: 64.2%GLaM: 71.7%\u201che\u201d examples: 70.8%\u201cshe\u201d examples: 72.5%stereotypical examples: 71.7%anti-stereotypical (\u201cgotcha\u201d) examples: 71.7%GLaM has less gender bias (as measured by this benchmark)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "FacebookMoE (Artetxe et al., 2021) Mixture of experts": {
        "paragraphs": [
            "Setup:",
            "",
            "Results on StereoSet:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Trained a 1.1T parameter model512 experts (more than GLaM), 32 layers, 4096 hidden unitsTrained on 112 billion tokens on webpages, forums, books, news, etc.Strong gains for smaller models, diminishing gains for larger models",
            "Example: The assistant went to work. {She brought her boss coffee., She was valued for her input.}Stereotype bias gets worse with increase model size (counterpoint to the GLaM results)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Decentralized mixture-of-experts (Ryabinin & Gusev, 2020) Mixture of experts": {
        "paragraphs": [
            "Motivation:",
            "Main considerations:",
            "Distributed hash tables:",
            "",
            "Experiments from the paper:",
            "Diskin et al., 2021:",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "So far, the mixture-of-experts was motivated purely from a perspective of a central organization (e.g., Google or Facebook) scaling up a massive large language model.However, mixture-of-experts naturally suggests a much more radical decentralization.The Azure supercomputer cluster used to train GPT-3 costs $250 million.How can we harness the hundreds of millions of consumer PCs?Folding@Home is a volunteer computing project that leverages volunteers across the world to donate compute to do molecular dynamics simulations.In April 2020, Folding@Home had 700,000 people donate compute producing 2.43 exaFLOPs (GPT-3 requires 350 gigaFLOPs) (article).The main difference is that molecular dynamics simulations is compute-heavy and doesn\u2019t require network bandwidth.",
            "Many nodes (\\(10^3 \\sim 10^6\\) heterogeneous PCs)Frequent node failures (5-20% have at least one failure/day)Home-Internet communication bandwidth (100Mbps; compared to 400Gbps for the Azure supercomputer)",
            "\\(N\\) nodesA single node needs to talk to \\(O(\\log N)\\) other nodesUsed Kademlia DHT protocol (used by BitTorrent and Ethereum)",
            "Top-4 experts (256 experts total)Each expert is a Transformer layerTrained a small Transformer LM on 4 GPUs",
            "40 volunteersTrained an ALBERT-style masked language model for BengaliTraining Transformers Together: anyone can join and contribute compute"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary Mixture of experts": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Mixture-of-experts: classic idea of applying different experts to different inputsAllows for training much larger language models (1.1 trillion parameters)Much more efficient per input (fewer FLOPs) than dense Transformer modelsHard to compare Direct comparisons are still challenging at scale (GPT-3 versus GLaM versus FacebookMoE)Strong implications for decentralization"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Retrieval-based models": {
        "paragraphs": [
            "We now turn to another class of language models, retrieval-based (or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer.",
            "Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework:",
            "Example (open-book question answering):",
            "Recall that BART and T5 are examples of encoder-decoder models:",
            "that are trained on denoising objectives; for example:",
            "Retrieval. Let us assume that we have a store \\(S\\), which is a set of sequences (usually, documents or passages).",
            "Intuitively, a retrieval-based model generates:",
            "Example (open-book question answering):",
            "Nearest neighbors as a special case:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Input \\(x\\): What is the capital of Canada?Output \\(y\\): Ottawa",
            "Input \\(x\\): Thank you  me to your party  week.Output \\(y\\):  for inviting  last",
            "Retrieve a relevant sequence(s) \\(z\\) based on input \\(x\\).Generate the output \\(y\\) given retrieved sequence(s) \\(z\\) and input \\(x\\).",
            "Input \\(x\\): What is the capital of Canada?Retrieval \\(z\\): Ottawa is the capital city of Canada.Output \\(y\\): Ottawa",
            "\\(S\\) is the training set.Retrieve the \\((x',y') \\in S\\) whose \\(x'\\) is most similar to \\(x\\).Generate \\(y = y'\\)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Retrieval-augmented generation (RAG) (Lewis et al., 2020) Retrieval-based models": {
        "paragraphs": [
            "",
            "Formally, the RAG-Sequence model is defined as follows:",
            "In practice, the summation \\(z \\in S\\) is replaced by the top-k (analogous to choosing the top 1 or 2 experts for mixture of experts).",
            "Retriever: Dense Passage Retrieval (DPR) (Karpukhin et al., 2020).",
            "Generator.",
            "Training.",
            "Experiments.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Considers on passages of 100 words with title of Wikipedia articleTrained on query, positive example, negative examples: \\((q, p^+, p^-_1, \\dots, p^-_n)\\) from QA datasets (NaturalQuestions, TriviQA, etc.)Negative passages: random + passages retrieved using BM25 on \\(q\\) that don\u2019t contain the answerInference: uses FAISS (Facebook AI Similarity Search)",
            "Use BART-large (400M parameters) where input is retrieved passage \\(z\\) concatenated with input \\(x\\)Recall BART was trained on denoising objectives (e.g., masking) on web, news, books, stories",
            "Initialize with BART, DPR (initialized with BERT).Tune \\(\\text{BART}\\) and \\(\\BERT_\\text{q}\\).",
            "Example of RAG-Token on Jeopardy question generation given input Hemingway:",
            "Outperforms non-retrieval methods:  For comparison, GPT-3 (few-shot): NaturalQuestions (29.9%), WebQuestions (41.5%), TriviaQA (71.2%)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "RETRO (Borgeaud et al., 2021) Retrieval-based models": {
        "paragraphs": [
            "Results:",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Retrieve based on chunks of 32 tokensStore: 2 trillion tokens7 billion parameters (25 times fewer parameters than GPT-3)Use frozen BERT for retrieval (don\u2019t update)Trained on MassiveText (same dataset used to train Gopher)",
            "Performs very well on language modelingNaturalQuestions accuracy: 45.5% (SOTA is 54.7%)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Discussion Retrieval-based models": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "The retrieval-based models are highly geared towards knowledge-intensive, question answering tasks.Beyond scalability, retrieval-based models provide interpretability and ability to update the store.Unclear whether these models have the same general-purpose capabilities as a dense Transformer."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "self-attention": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "feed-forward": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "dense Transformer": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "parallelism": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "limits": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "network bandwidth": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "\u201cselective\u201d architectures": {
        "paragraphs": [
            "In this lecture, we will explore two different types of \u201cselective\u201d architectures, which raises the ceiling of how big the models can get. In particular, we will discuss:",
            "Current state of affairs:",
            "Basics. The idea of mixture of experts goes back to Jacobs et al. (1991)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Mixture-of-experts": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "experts": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Retrieval": {
        "paragraphs": [
            "Retrieval. Let us assume that we have a store \\(S\\), which is a set of sequences (usually, documents or passages).",
            "that are trained on denoising objectives; for example:",
            "Intuitively, a retrieval-based model generates:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "store": {
        "paragraphs": [
            "Retrieval. Let us assume that we have a store \\(S\\), which is a set of sequences (usually, documents or passages).",
            "that are trained on denoising objectives; for example:",
            "Intuitively, a retrieval-based model generates:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Basics": {
        "paragraphs": [
            "Basics. The idea of mixture of experts goes back to Jacobs et al. (1991).",
            "In this lecture, we will explore two different types of \u201cselective\u201d architectures, which raises the ceiling of how big the models can get. In particular, we will discuss:",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "mixture-of-experts": {
        "paragraphs": [
            "But the mixture-of-experts approach is to:",
            "where the parameters are \\(\\theta = (W_1, W_2)\\).",
            "Example. Consider \\(d = 2\\) and each expert being a linear classifier (source):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "gating function": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "expert function": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Example": {
        "paragraphs": [
            "Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts:",
            "For example, we can take \\(\\lambda = \\frac{0.01}{B}\\).",
            "The counter would be:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Saving compute": {
        "paragraphs": [
            "Saving compute.",
            "Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts.",
            "Balancing experts."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "approximate": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Balancing experts": {
        "paragraphs": [
            "Balancing experts.",
            "Notation:",
            "For example, we can take \\(\\lambda = \\frac{0.01}{B}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "all the experts are used": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Parallelism": {
        "paragraphs": [
            "Parallelism.",
            "Balancing experts.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "activated": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "mixture-of-experts (MoE) feed-forward network": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "top-2 experts": {
        "paragraphs": [
            "We define the top-2 experts approximate gating function as follows:",
            "",
            "Notation:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Overflow": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Auxiliary loss": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "load balancing is a constraint": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "At test time": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Summary and next steps": {
        "paragraphs": [
            "Summary and next steps.",
            "BASE requires more compute to optimize the assignment \\(a\\), but is more stable.",
            "Specification:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "{her/his,their}": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "central": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "decentralization": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "volunteer computing": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "larger": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "efficient": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "retrieval-based": {
        "paragraphs": [
            "We now turn to another class of language models, retrieval-based (or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer.",
            "",
            "Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Retrieve": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Generate": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Nearest neighbors": {
        "paragraphs": [
            "Nearest neighbors as a special case:",
            "Example (open-book question answering):",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "top-k": {
        "paragraphs": [
            "In practice, the summation \\(z \\in S\\) is replaced by the top-k (analogous to choosing the top 1 or 2 experts for mixture of experts).",
            "Formally, the RAG-Sequence model is defined as follows:",
            "Retriever: Dense Passage Retrieval (DPR) (Karpukhin et al., 2020)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Retriever: Dense Passage Retrieval (DPR)": {
        "paragraphs": [
            "Retriever: Dense Passage Retrieval (DPR) (Karpukhin et al., 2020).",
            "In practice, the summation \\(z \\in S\\) is replaced by the top-k (analogous to choosing the top 1 or 2 experts for mixture of experts).",
            "Generator."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Generator": {
        "paragraphs": [
            "Generator.",
            "Retriever: Dense Passage Retrieval (DPR) (Karpukhin et al., 2020).",
            "Training."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Experiments": {
        "paragraphs": [
            "Experiments.",
            "Training.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "chunks": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "interpretability": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "REALM": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "RAG": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "RETRO": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Why adapt the language model?": {
        "paragraphs": [
            "The format of such a task may not be very natural for the model."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "LMs are trained in a task-agnostic way.Downstream tasks can be very different from language modeling on the Pile.For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):Premise: I have never seen an apple that is not red.Hypothesis: I have never seen an apple.Correct output: Not entailment (the reverse direction would be entailment)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Ways downstream tasks can be different Why adapt the language model?": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Formatting: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.Topic shift: the downstream task is focused on a new or very specific topic (e.g., medical records)Temporal shift: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "General adaptation setup": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "In the adaptation phase, we train a new model that depends on pre-trained LM parameters \\(\\theta_{\\text{LM}}\\) that parameterize the LM \\(p\\).We are given a downstream dataset \\((x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)})\\) sampled from a downstream task distribution \\(P_{\\text{task}}\\).We minimize some parameters \\(\\gamma\\) from a family of parameters \\(\\Gamma\\) on a task loss \\(\\ell_{\\text{task}}\\) (e.g., cross entropy loss).The family of parameters \\(\\Gamma\\) may represent a subset of the existing parameters or introduce new parameters.The output of the optimization problem are the adapted parameters \\(\\gamma_{\\text{adapt}}\\), which parameterizes the adapted model \\(p_{\\text{adapt}}\\):"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Probing": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Strategies for fixed-length representations Probing": {
        "paragraphs": [
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "CLS token Devlin et al. 2018: During both pre-training and fine-tuning, we prepend a special token called CLS to the prompt. We use the embedding vector corresponding to the CLS token as the \u201csequence-level\u201d embedding.Average over tokens: Another common way is to average over the \\(L\\) tokens. Note: this does not make the model permutation-invariant, since each embedding vector is contextual and position-dependent.",
            "Summary",
            "Freeze (gray): language model representation encoderOptimize (blue, changes per task): probe (prediction head)Models: linear or shallow feedforward prediction head"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Fine-tuning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Fine-tuning for zero-shot performance Fine-tuning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "FLAN and T0 fine-tune the model for better zero-shot performance.They unify the prompt format of many downstream tasks and fine-tune the model to perform diverse tasks with this formatting.Zero-shot performance on unseen tasks improves over the original language model.The model is learning to use the prompt format to do zero-shot tasks."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Fine-tuning for human-aligned language models Fine-tuning": {
        "paragraphs": [
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Given instructions in a prompt, LMs should produce outputs that are helpful (useful for the user), honest (don\u2019t mislead the user), and harmless (doesn\u2019t cause physical, psychological, or social harm).Language modeling is not inherently aligned with these goals.InstructGPT aligns the LM (GPT-3) with 3 steps:1) Collect human-written demonstrations of desired behavior. Do supervised fine-tuning on demonstrations.2) On a set of instructions, sample \\(k\\) outputs from the LM from step 1 for each instruction. Gather human preferences for which sampled output is most preferred - this data is cheaper to collect than step 1.3) Fine-tune the LM from step 1 with a reinforcement learning objective to maximize the human preference reward.A 1.3B InstructGPT model produces outputs that are preferred to 175B GPT-3 85% of the time, and 71% when using few-shot prompts with GPT-3.On closed-domain QA/summarization, InstructGPT hallucinates information 21% of the time vs 41% in GPT-3InstructGPT generates 25% fewer toxic outputs than GPT-3 when prompted to be respectfulInstructGPT doesn\u2019t improve bias: not much benefit on Winogender and CrowSPairs",
            "Summary",
            "Freeze (gray): nothingOptimize (blue, changes per task): all parameters of the language model, plus a new prediction head"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Lightweight Fine-tuning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Lightweight fine-tuning aims to have the expressivity of full fine-tuning while not requiring us to store the full language model for every task.Many lightweight fine-tuning variants - amongst them we discuss prompt tuning, prefix tuning, and adapter tuning."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Prompt tuning [Lester et al., 2021] Lightweight Fine-tuning": {
        "paragraphs": [
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Developed for text classification tasks on the T5 model (an encoder-decoder)Motivated by prompt design/engineering in inference-based adaptation, prompt tuning prepends \\(k\\) learnable, continuous token embeddings (this defines \\(\\Gamma\\))to the input (so the input is now length \\(L\u2019=L+k\\)) and trains this on the labeled task data. The entire pre-trained language model is frozen.Scaling improves prompt tuning: with larger frozen language models, prompt tuning\u2019s performance becomes more competitive with full fine-tuning (\u201cmodel tuning\u201d).Learned prompt embedding initialization strategies:Embeddings of random vocab wordsEmbeddings of class label wordsRandom init: doesn\u2019t work well"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Prefix tuning [Li and Liang, 2021] Lightweight Fine-tuning": {
        "paragraphs": [
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices.",
            "where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention.",
            "Trainable parameters at all layers helps"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Developed for language generation tasks on the BART and GPT-2 modelsFor \\(k\\) positions prepended to the input, concatenate additional learnable weights for keys and values at every attention layer. Different to prompt tuning (only learnable input vectors).To define prefix tuning, we use a generalized definition of an attention operation, which takes in 3 arguments: the key \\(K \\in \\R^{d\\times L\u2019}\\), value \\(V \\in \\R^{d \\times L\u2019}\\), and query \\(Q \\in \\R^{d \\times L}\\):",
            "For attention head \\(i\\), prefix tuning computes attention with a larger \\(L\u2019 = L + k\\) by concatenating learnable weights \\(P_{\\text{key}}^{(i)} , P_{\\text{value}}^{(i)} \\in \\R^{d \\times k}\\) to the key and value He et al. 2022:",
            "Prompt tuning v2 is the all-layer version of prompt tuning.All layer parameters seem to help for text classification and generation."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Adapter tuning [Houlsby et al. 2019] Lightweight Fine-tuning": {
        "paragraphs": [
            "where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Add a new learned \u201cbottleneck\u201d layer (adapters) between each (frozen) Transformer layerAdapters are usually 2-layer residual networks that operate on each element \\(x \\in \\R^d\\) of the sequence individually:",
            "Note: How expressive is lightweight fine-tuning? Complex since expressivity is tied to the particular pre-trained LM - if the pre-trained LM has weights that are 0, then prompt/prefix tuning would not do anything."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Parallelization over prefixes/prompts Lightweight Fine-tuning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Suppose we want to deploy a personalized model for \\(N\\) users.With prefix tuning, we can store \\(N\\) prefixes, one for each userRun personalized model for each user in parallel across a minibatch by prepending the corresponding user-specific prefix to each input in the minibatch."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Robustness of lightweight fine-tuning Lightweight Fine-tuning": {
        "paragraphs": [
            "",
            "",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Lightweight fine-tuning methods tend to improve out-of-distribution (OOD) performance compared to full fine-tuning, for example on text from different topics or domains.Prompt tuning improves OOD accuracy: Table of F1 results from prompt tuning vs full fine-tuning (Model tuning) trained on SQuAD and tested on out-of-domain MRQA 2019 tasks",
            "Prefix tuning improves OOD accuracy: on the XSUM summarization task, where models are fine-tuned on news articles and tested on sports (news-to-sports) or trained on {world, UK, business} articles and tested on {health, technology} articles (within-news).For XSUM, the metric used is ROUGE-L, an automatic evaluation metric for summarization based on matching length \\(L\\) subsequences with a reference summarization. Note that when the test dataset is not out-of-distribution, prefix tuning\u2019s accuracy is usually slightly worse than full fine-tuning.",
            "Summary",
            "Freeze (gray): whole/most of language modelOptimize (blue, changes per task): small number of additional parameters (<1% of the parameters)Methods: prompt tuning, prefix tuning, adapter tuning, and others (LoRA, BitFit, \u2026)"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Overall summary": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "We need to adapt large language models to the diverse array of downstream tasks, which may be very different from language modeling.Probing trains a task-specific prediction head on top of a frozen language model, treating the language model as a good representation extractor. Freezing the language model tends to limit the expressivity of the method.Fine-tuning treats the large language model parameters as initialization for further training all of the parameters on a downstream task, which is much more expressive than probing but more expensive since we have to save the whole model for each downstream task.Lightweight fine-tuning strikes a balance between fine-tuning and probing by optimizing only a few parameters (<%1 of the model), but it optimizes high-leverage parts of the model so that it is still very expressive."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Premise": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Hypothesis": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Correct output": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Formatting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Topic shift": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Temporal shift": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "CLS token Devlin et al. 2018": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Average over tokens": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Freeze": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Optimize": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Methods": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Lightweight fine-tuning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "T0": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "FLAN": {
        "paragraphs": [
            "Finetuned Language Models Are Zero-Shot Learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le. 2021. Introduces FLAN from Google."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Life cycle assessment": {
        "paragraphs": [
            "This section is largely based on Ligozat et al. (2021).",
            "Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about:",
            "Life cycle assessment (LCA).",
            "Life cycle of IT equipment:",
            "Considerations in the life cycle:",
            "The \u2018Use\u2019 stage:",
            "",
            "Environmental impact:",
            "Other second-order effects (more details):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "the full environmental impact (emissions, water footprint) ofthe full lifecycle of the IT equipment (e.g., production, use, end of life).",
            "The Life cycle assessment (LCA) (ISO 14040 and 14044) offers a framework to do this.Need to \u201ctake a systems perspective\u201d to avoid \u201csolutions to one problem creates several new and often ignored problems\u201d.",
            "Production:Raw material extraction: all processes to extract ore and convert into metalsManufacturing: includes all processes that create the equipmentTransport: shipping the equipmentUse: actual energy consumption of the actual equipmentEnd of life: dismantle, recycle/dispose of equipment",
            "ProductionWe don\u2019t have life cycle assessments for GPUs/TPUsCPU only data center in France: 40% of GHG emissions were due to the production phase (Berthoud et al. 2020)A data center is built for many purposes, need to do credit assignment to determine what share is due to large language models (hard to estimate in advance, since things change rapidly).Example: Manufacturing accounts for 75% of the total emissions of an iPhone 5UseMostly depends on carbon intensity of energy sources (coal or hydroelectric)End of lifeGenerally not well documented80% of electronic equipment is not formally collected",
            "Data: need to acquire, produce, and store dataLearning: invest in training a large language modelThis includes experimentation and hyperparameter tuning.This is a \u201cone-time cost\u201d\u2026until you need to update the model again.Inference: running the model in productionExample: Google gets 5.6 billion search queries a day (source).Often what is deployed is a much smaller model that is distilled from the large model (which can be a lot smaller if task-specific).If you need to make multiple predictions (e.g., sentiment, topic classification, etc.), can encode sentence once (e.g., BERT) and use different task-specific heads.",
            "Greenhouse gas emissions: leads to climate changeWater footprint: freshwater is a scarce resource in some regionsData center uses water for cooling (which requires electricity)Electricity generation is the second largest water consumer, and treating water and waste water requires electricityHuman toxicity: chemicals that are released into the environment (air, water, soil) that can cause cancer, etc.Chip manufacturing create toxic waste sites in Silicon ValleyAbiotic resource depletionFossil fuelsMinerals (lithium, cobalt) used to manufacture electronic devices You can learn more about the environmental impact of data centers.",
            "More efficiency creates more demand (rebound effect and Jevon\u2019s paradox)Environmental change (accelerated desertification, increased extinction rates)Melting permafrost in turn accelerates greenhouse gas emissionsChip shortages lead to stoppages in automobile manufacturing"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Climate change": {
        "paragraphs": [
            "Climate change. On one hand, we\u2019ve all heard about the very serious dangers of climate change (article, article):",
            "\\(\\newcommand\\pcpu{p_\\text{cpu}} \\newcommand\\pgpu{p_\\text{gpu}} \\newcommand\\pdram{p_\\text{dram}} \\newcommand\\pue{\\text{PUE}} \\newcommand\\emissions{\\text{emissions}} \\newcommand\\emissionsPerPower{R_{\\text{power} \\to \\text{emit}}}\\) In this lecture, ask the question: what is the environmental impact of large language models?",
            "Large language models. On the other hand, we see a massive increase in the amount of compute required to train large language models (and therefore contributing to emissions). Here are some example data points:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Energy use and greenhouse gas emissions": {
        "paragraphs": [
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated.",
            "Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)",
            "",
            "From Lacoste et al. (2019):",
            "de Chalendar et al. 2019",
            "Data centers statistics (Md Abu Bakar Siddik et al., 2021):"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "Fossil fuels (coal, gas) produce the most emissions (from direct emissions)Other green energy (solar, wind) also have emissions if take into account the entire lifecycle (construction of power plants, mining, waste management)Running same job in Quebec (hydroelectric) would have 30x less emissions than Estonia (coal)",
            "Depends on location (what kind of power plants are there)Depends on temporal effects (seasons, time of day)Electricity exchanges means its harder to keep track and the negative effects are often elsewhere40% of emissions in California\u2019s main Balancing Authority (BA) were produced elsewhere",
            "Globally, data centers use 205 billion kWh of electricity in 2018 (1% of total electricity use).In the United States, data centers use 1.8% of electricity in 2014.30% of all data centers are in the United States.0.5% of total US greenhouse gas emissions are attributable to data centers.Good news: Computing workloads have increased 550% from 2010 to 2018, but electricity consumption increased by only 6% (due to improvements in energy efficiency)."
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Estimating emissions for training models": {
        "paragraphs": [
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs.",
            "ML CO2 Impact Calculator (Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Strubell et al., 2018 Estimating emissions for training models": {
        "paragraphs": [
            "This was the first paper to really spark awareness of environmental impact within the NLP community.",
            "Compute power use in kWh:",
            "They used average values:",
            "Results."
        ],
        "tables": [],
        "links": [],
        "equations": [
            "\\(\\pcpu\\): average power (W) from CPUs\\(\\pgpu\\): average power (W) from GPUs\\(\\pdram\\): average power (W) from DRAM\\(\\pue\\): Power usage effectiveness: total power supplied to data center / power consumed by IT equipment",
            "\\(\\pue = 1.58\\) (2018 global average for data centers)\\(\\emissionsPerPower = 0.954\\) (2018 average emissions - pounds per kWh)",
            "BERT-base (110M parameters): 1438 lbs CO2eqNVIDIA trains in 79.2 hours on 64 V100 GPUsNeural architecture search (213M parameters) to obtain Evolved Transformer So et al. (2019): 626,155 lbs CO2eqBase model takes 10 hours to train (300K steps) on one TPUv2Conclude takes 32,623 hours to train (979M steps)1 passenger on a round trip flight from New York to San Francisco: 1984 lbs CO2eq (0.9 tCO2eq)Lifetime of a car: 126,000 lbs CO2eq"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Patterson et al., 2021": {
        "paragraphs": [
            "Simple formula:",
            "Many design decisions",
            "",
            "For training:",
            "Estimates of models:",
            "Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate:",
            "Points:"
        ],
        "tables": [],
        "links": [],
        "equations": [
            "NVIDIA: 80% of the ML workload is inference, not training",
            "Model architecture: Transformer versus Evolved TransformerProcessor: NVIDIA\u2019s P100 versus Google TPUsData center: average (1.58) versus Google\u2019s (1.11)Energy supply mix (e.g., coal, hydroelectric): average (0.429 kg CO2eq / kWh) versus Google\u2019s (0.080 kg CO2eq / kWh)Note: gross is 0.478, net is 0.080Deduct the clean energy sold to other companies",
            "T5: 86 MWh, 47 tCO2eqGShard (mixture of experts for machine translation): 24 MWh, 4.3 net tCO2eqSwitch Transformer: 179 MWh, 59 tCO2eqGPT3: 1287 MWh, 552 tCO2eq",
            "Small proxy task to search, so 18.7x too highNeural architecture search is done once, and everyone can now use Evolved TransformerOverestimated emissions by 88x",
            "Measurement is better than online calculators if possibleGoogle used 12.2 tWh (training Google\u2019s 4 largest models less than 0.005%)This is 1/10 of compute spent on bitcoin mining"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Python packages": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [
            "Environment Impact TrackerCarbon TrackerCodeCarbon"
        ],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "holistic understanding": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "calculate the emissions": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "awareness": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "costs": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "tradeoff": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "estimates": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Philosophy": {
        "paragraphs": [
            "Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about:",
            "This section is largely based on Ligozat et al. (2021).",
            "Life cycle assessment (LCA)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "full environmental impact": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "full lifecycle": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Life cycle assessment (LCA)": {
        "paragraphs": [
            "Life cycle assessment (LCA).",
            "Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about:",
            "Life cycle of IT equipment:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "systems perspective": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Production": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Use": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "End of life": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Learning": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Inference": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Temperatures": {
        "paragraphs": [
            "Temperatures are rising:",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on.",
            "Negative impacts:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generating electricity": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "kg CO2 eq": {
        "paragraphs": [
            "Measurement of carbon emissions is kg CO2 eq:",
            "Causes:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "global warming potential": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Carbon intensity": {
        "paragraphs": [
            "Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated.",
            ""
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data centers": {
        "paragraphs": [
            "Data centers statistics (Md Abu Bakar Siddik et al., 2021):",
            "de Chalendar et al. 2019",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Mitigation": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Reporting": {
        "paragraphs": [],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    }
}